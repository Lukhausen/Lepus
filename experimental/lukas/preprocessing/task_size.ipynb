{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Finding Top 5 Tasks by JSON String Length in 'modded_tasks' ---\n",
      "  - Metric 1: Length(JSON(Train)) + Length(JSON(Test Input))\n",
      "  - Metric 2: Length(JSON(Test Output))\n",
      "Scanning 30138 task files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning tasks: 100%|██████████| 30138/30138 [00:32<00:00, 914.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scan Complete ---\n",
      "\n",
      "--- Top 5 Task Files: Longest JSON Length (Train + Test Input) ---\n",
      "  1. f9d67f8b_r270_mh_c_p2c1_p2c8.json\n",
      "  2. f9d67f8b_mh_p2c5_p1c4.json\n",
      "  3. f9d67f8b_r180_mh_p2c3_p1c0.json\n",
      "  4. f9d67f8b_c_p2c4.json\n",
      "  5. f9d67f8b_r270_c_p1c5_p1c9.json\n",
      "\n",
      "--- Top 5 Task Files: Longest JSON Length (Test Output) ---\n",
      "  1. 05a7bcf2_c_p2c9.json\n",
      "  2. 05a7bcf2_mh_c_p2c1.json\n",
      "  3. 05a7bcf2_r180_mh_c_p2c7.json\n",
      "  4. 09c534e7_r90_mh_p2c8.json\n",
      "  5. 25094a63_p1c0_p2c5.json\n",
      "\n",
      "--- Scan Statistics ---\n",
      "- Files scanned: 30138\n",
      "- Files missing data for Metric 1 (train/test-in): 0\n",
      "- Files missing data for Metric 2 (test-out):      0\n",
      "- Files with read/parse errors:                   0\n",
      "- Valid test output grids found for stats:        30138\n",
      "- Average test output grid dimensions (W x H):    12.93 x 12.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import operator # For sorting tuples\n",
    "\n",
    "# --- Configuration ---\n",
    "# Ensure this path points to your augmented tasks directory\n",
    "output_dir_path = \"modded_tasks\"\n",
    "top_n = 5 # How many top tasks to rank\n",
    "# --- End Configuration ---\n",
    "\n",
    "output_dir = Path(output_dir_path)\n",
    "\n",
    "# --- Data Structures for Rankings ---\n",
    "# Store tuples: (total_char_length, filename)\n",
    "metric1_results = [] # For Train (JSON) + Test Input (JSON) length\n",
    "metric2_results = [] # For Test Output (JSON) length\n",
    "\n",
    "# --- Counters & Statistics ---\n",
    "files_scanned = 0\n",
    "files_with_errors = 0\n",
    "files_without_metric1_data = 0\n",
    "files_without_metric2_data = 0\n",
    "\n",
    "# For average test output dimensions\n",
    "total_test_out_width = 0\n",
    "total_test_out_height = 0\n",
    "valid_test_out_grid_count = 0\n",
    "\n",
    "\n",
    "print(f\"\\n--- Finding Top {top_n} Tasks by JSON String Length in '{output_dir.name}' ---\")\n",
    "print(f\"  - Metric 1: Length(JSON(Train)) + Length(JSON(Test Input))\")\n",
    "print(f\"  - Metric 2: Length(JSON(Test Output))\")\n",
    "\n",
    "if not output_dir.exists():\n",
    "    print(f\"Error: Output directory '{output_dir}' does not exist.\")\n",
    "else:\n",
    "    task_files = sorted(list(output_dir.glob(\"*.json\")))\n",
    "    if not task_files:\n",
    "        print(\"No .json files found in the output directory.\")\n",
    "    else:\n",
    "        print(f\"Scanning {len(task_files)} task files...\")\n",
    "        for task_path in tqdm(task_files, desc=\"Scanning tasks\"):\n",
    "            files_scanned += 1\n",
    "            can_calc_metric1 = False\n",
    "            can_calc_metric2 = False\n",
    "            metric1_len = 0\n",
    "            metric2_len = 0\n",
    "            test_output_grid_for_dims = None # Store grid temporarily for dimension calc\n",
    "\n",
    "            try:\n",
    "                with open(task_path, 'r') as f:\n",
    "                    task_data = json.load(f)\n",
    "\n",
    "                # --- Try to get data for Metric 1 ---\n",
    "                train_data = task_data.get('train')\n",
    "                test_examples = task_data.get('test', [])\n",
    "                test_input_grid = None\n",
    "\n",
    "                if train_data is not None and test_examples:\n",
    "                    test_input_grid = test_examples[0].get('input')\n",
    "                    if test_input_grid is not None:\n",
    "                        train_string = json.dumps(train_data)\n",
    "                        test_input_string = json.dumps(test_input_grid)\n",
    "                        metric1_len = len(train_string) + len(test_input_string)\n",
    "                        can_calc_metric1 = True\n",
    "\n",
    "                # --- Try to get data for Metric 2 & Dimensions ---\n",
    "                # Reuse test_examples from above\n",
    "                if test_examples:\n",
    "                    test_output_grid_for_dims = test_examples[0].get('output') # Get the output grid\n",
    "                    if test_output_grid_for_dims is not None:\n",
    "                        # Calculate length for ranking\n",
    "                        test_output_string = json.dumps(test_output_grid_for_dims)\n",
    "                        metric2_len = len(test_output_string)\n",
    "                        can_calc_metric2 = True\n",
    "\n",
    "                        # Calculate dimensions for averages if it's a valid grid structure\n",
    "                        if (isinstance(test_output_grid_for_dims, list) and\n",
    "                            test_output_grid_for_dims and # not empty list\n",
    "                            isinstance(test_output_grid_for_dims[0], list) and\n",
    "                            test_output_grid_for_dims[0]): # first row not empty list\n",
    "                            height = len(test_output_grid_for_dims)\n",
    "                            width = len(test_output_grid_for_dims[0])\n",
    "                            if height > 0 and width > 0: # Check dimensions > 0\n",
    "                                total_test_out_height += height\n",
    "                                total_test_out_width += width\n",
    "                                valid_test_out_grid_count += 1\n",
    "\n",
    "\n",
    "                # --- Store results if calculated ---\n",
    "                if can_calc_metric1:\n",
    "                    metric1_results.append((metric1_len, task_path.name))\n",
    "                else:\n",
    "                    files_without_metric1_data += 1\n",
    "\n",
    "                if can_calc_metric2:\n",
    "                    metric2_results.append((metric2_len, task_path.name))\n",
    "                else:\n",
    "                    # This counter increments if test_examples is empty OR test_output_grid is None\n",
    "                    files_without_metric2_data += 1\n",
    "\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                files_with_errors += 1\n",
    "                files_without_metric1_data += 1\n",
    "                files_without_metric2_data += 1\n",
    "            except Exception as e:\n",
    "                files_with_errors += 1\n",
    "                files_without_metric1_data += 1\n",
    "                files_without_metric2_data += 1\n",
    "\n",
    "        print(\"\\n--- Scan Complete ---\")\n",
    "\n",
    "        # --- Sort and Rank ---\n",
    "        metric1_results.sort(key=operator.itemgetter(0), reverse=True)\n",
    "        metric2_results.sort(key=operator.itemgetter(0), reverse=True)\n",
    "\n",
    "        top_metric1 = metric1_results[:top_n]\n",
    "        top_metric2 = metric2_results[:top_n]\n",
    "\n",
    "        # --- Report Metric 1 Ranking (Filenames Only) ---\n",
    "        print(f\"\\n--- Top {len(top_metric1)} Task Files: Longest JSON Length (Train + Test Input) ---\")\n",
    "        if top_metric1:\n",
    "            for i, (_, filename) in enumerate(top_metric1): # Unpack tuple, ignore length\n",
    "                print(f\"  {i+1}. {filename}\")\n",
    "        else:\n",
    "            print(\"  No suitable tasks found for this metric.\")\n",
    "\n",
    "        # --- Report Metric 2 Ranking (Filenames Only) ---\n",
    "        print(f\"\\n--- Top {len(top_metric2)} Task Files: Longest JSON Length (Test Output) ---\")\n",
    "        if top_metric2:\n",
    "            for i, (_, filename) in enumerate(top_metric2): # Unpack tuple, ignore length\n",
    "                print(f\"  {i+1}. {filename}\")\n",
    "        else:\n",
    "            print(\"  No suitable tasks found for this metric.\")\n",
    "\n",
    "        # --- Calculate and Report Average Test Output Dimensions ---\n",
    "        avg_width = 0\n",
    "        avg_height = 0\n",
    "        if valid_test_out_grid_count > 0:\n",
    "            avg_width = total_test_out_width / valid_test_out_grid_count\n",
    "            avg_height = total_test_out_height / valid_test_out_grid_count\n",
    "\n",
    "        # --- Report Stats ---\n",
    "        print(f\"\\n--- Scan Statistics ---\")\n",
    "        print(f\"- Files scanned: {files_scanned}\")\n",
    "        print(f\"- Files missing data for Metric 1 (train/test-in): {files_without_metric1_data}\")\n",
    "        print(f\"- Files missing data for Metric 2 (test-out):      {files_without_metric2_data}\")\n",
    "        print(f\"- Files with read/parse errors:                   {files_with_errors}\")\n",
    "        print(f\"- Valid test output grids found for stats:        {valid_test_out_grid_count}\")\n",
    "        if valid_test_out_grid_count > 0:\n",
    "            print(f\"- Average test output grid dimensions (W x H):    {avg_width:.2f} x {avg_height:.2f}\")\n",
    "        else:\n",
    "            print(\"- Average test output grid dimensions (W x H):    N/A (No valid grids found)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
