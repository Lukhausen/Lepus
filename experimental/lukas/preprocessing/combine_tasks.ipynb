{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ARC JSON Tasks to Custom JSONL Format\n",
    "\n",
    "This notebook converts a folder of ARC task `.json` files into a JSON Lines (`.jsonl`) file where each line is a JSON object with three fields:\n",
    "\n",
    "- **train**: A string with formatted training examples (inputs and outputs).\n",
    "- **test**: A string with formatted test inputs.\n",
    "- **test_answer**: The test answer(s) as the raw grid arrays (if only one test example is present, then the output grid is provided directly; if multiple, a list of grids is provided).\n",
    "\n",
    "The notebook is now reorganized into several helper functions:\n",
    "\n",
    "1. **get_train_string**: Returns the formatted training examples string.\n",
    "2. **get_test_string**: Returns the formatted test inputs string.\n",
    "3. **get_test_answer**: Returns the ground truth output(s) from test examples.\n",
    "4. **format_task_file_to_strings**: Returns a tuple of all three strings for one task file.\n",
    "\n",
    "Then the notebook tests the functions on a single task file and finally loops over all files in a folder to create the JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# ## 1. Setup: Import Libraries\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm  # for progress visualization\n",
    "import traceback  # for improved error reporting\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_grid_to_string(grid):\n",
    "    \"\"\"\n",
    "    Converts a grid (list of lists) into a nested list string format.\n",
    "    Each row becomes a list, and line breaks appear only after each inner list.\n",
    "\n",
    "    Args:\n",
    "        grid (list[list[int]]): The input grid.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted nested list as a string with minimal line breaks.\n",
    "    \"\"\"\n",
    "    if not grid or not isinstance(grid, list) or not isinstance(grid[0], list):\n",
    "        return \"\"\n",
    "    inner_lists = [f\"[{','.join(map(str, row))}],\" for row in grid]\n",
    "    return \"[\\n\" + \"\\n\".join(inner_lists) + \"\\n]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom formatting functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ## 3. Define Formatting Functions for Each Component\n",
    "\n",
    "def get_train_string(task_data, train_template):\n",
    "    \"\"\"\n",
    "    Formats the training examples using the provided template.\n",
    "    \n",
    "    Args:\n",
    "        task_data (dict): The loaded JSON data for a task.\n",
    "        train_template (str): Template for training examples. Expected placeholders: {index}, {input}, {output}.\n",
    "    \n",
    "    Returns:\n",
    "        str: The formatted training examples.\n",
    "    \"\"\"\n",
    "    train_examples = task_data.get(\"train\", [])\n",
    "    formatted_examples = []\n",
    "    for idx, example in enumerate(train_examples):\n",
    "        input_str = format_grid_to_string(example.get(\"input\"))\n",
    "        output_str = format_grid_to_string(example.get(\"output\"))\n",
    "        formatted_examples.append(\n",
    "            train_template.format(index=idx + 1, input=input_str, output=output_str)\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted_examples)\n",
    "\n",
    "\n",
    "def get_test_string(task_data, test_template):\n",
    "    \"\"\"\n",
    "    Formats the test inputs using the provided template.\n",
    "    \n",
    "    Args:\n",
    "        task_data (dict): The loaded JSON data for a task.\n",
    "        test_template (str): Template for test examples. Expected placeholders: {index}, {input}.\n",
    "    \n",
    "    Returns:\n",
    "        str: The formatted test input examples.\n",
    "    \"\"\"\n",
    "    test_examples = task_data.get(\"test\", [])\n",
    "    formatted_examples = []\n",
    "    for idx, example in enumerate(test_examples):\n",
    "        input_str = format_grid_to_string(example.get(\"input\"))\n",
    "        formatted_examples.append(\n",
    "            test_template.format(index=idx + 1, input=input_str)\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted_examples)\n",
    "\n",
    "\n",
    "def get_test_answer(task_data):\n",
    "    \"\"\"\n",
    "    Extracts the ground truth output(s) from the test examples.\n",
    "    If there is only one test example with an output, returns it directly; otherwise returns a list of outputs.\n",
    "    \n",
    "    Args:\n",
    "        task_data (dict): The loaded JSON data for a task.\n",
    "    \n",
    "    Returns:\n",
    "        object: The formatted test output or list of outputs.\n",
    "    \"\"\"\n",
    "    test_examples = task_data.get(\"test\", [])\n",
    "    answers = []\n",
    "    for example in test_examples:\n",
    "        answers.append(example.get(\"output\", []))\n",
    "    return answers[0] if len(answers) == 1 else answers\n",
    "\n",
    "\n",
    "def format_task_file_to_strings(json_path, train_template, test_template):\n",
    "    \"\"\"\n",
    "    Loads a JSON task file and returns a tuple with the three formatted components:\n",
    "    (train_string, test_string, test_answer).\n",
    "    \n",
    "    Args:\n",
    "        json_path (str or Path): Path to the JSON file.\n",
    "        train_template (str): Template for formatting training examples.\n",
    "        test_template (str): Template for formatting test inputs.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_string, test_string, test_answer) or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_path = Path(json_path)\n",
    "        with open(json_path, 'r', encoding='utf-8') as infile:\n",
    "            task_data = json.load(infile)\n",
    "        \n",
    "        train_str = get_train_string(task_data, train_template)\n",
    "        test_str = get_test_string(task_data, test_template)\n",
    "        test_answer = get_test_answer(task_data)\n",
    "        \n",
    "        return train_str, test_str, test_answer\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing file {json_path.name}: {e}\")\n",
    "        # Uncomment below to see the full traceback if needed\n",
    "        # traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Custom formatting functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing custom formatting for: 8dab14c2_test1.json ---\n",
      "\n",
      "--- TRAIN ---\n",
      "\n",
      "### Train Example 1:\n",
      "Input:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,6,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,5,6,6,6,6,6,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,5,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "Output:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,5,5,5,5,6,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,5,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,5,5,5,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "### Train Example 2:\n",
      "Input:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "Output:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,5,5,5,5,5,5,5,5,5,5,5,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,6,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "### Train Example 3:\n",
      "Input:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,5,6,6,6,6,6,5,6,6,6,6],\n",
      "[6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "Output:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,5,6,6,6,6,6,5,6,6,6,6],\n",
      "[6,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,5,5,5,6,5,5,5,5,5,6,5,5,5,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "### Train Example 4:\n",
      "Input:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,6,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "Output:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,6,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,6,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,5,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "--- TEST ---\n",
      "\n",
      "### Test Input:\n",
      "[\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,5,5,5,5,5,5,5,5,5,6,6],\n",
      "[6,6,5,5,5,6,5,6,6,6,5,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,5,5,5,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\n",
      "]\n",
      "\n",
      "--- TEST_ANSWER ---\n",
      "\n",
      "[[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 5, 5, 5, 5, 6, 5, 5, 5, 6, 5, 5, 5, 6, 6], [6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6], [6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6], [6, 6, 5, 5, 5, 6, 5, 6, 6, 6, 5, 6, 6, 6, 6, 6], [6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]]\n",
      "\n",
      "--- End of Test ---\n"
     ]
    }
   ],
   "source": [
    "# ## 4. Test the Function on a Single Task File\n",
    "\n",
    "# Define custom templates for train and test inputs\n",
    "train_template = (\n",
    "    \"### Train Example {index}:\\n\"\n",
    "    \"Input:\\n\"\n",
    "    \"{input}\\n\\n\"\n",
    "    \"Output:\\n\"\n",
    "    \"{output}\"\n",
    ")\n",
    "\n",
    "test_template = (\n",
    "    \"### Test Input:\\n\"\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "# Set the sample task filename (adjust the folder/filename as needed)\n",
    "test_input_folder = \"modded_tasks\"  # Folder containing your ARC JSON task files\n",
    "example_task_filename = \"8dab14c2_test1.json\"  # Example file name\n",
    "\n",
    "example_task_path = Path(test_input_folder) / example_task_filename\n",
    "\n",
    "print(f\"--- Testing custom formatting for: {example_task_path.name} ---\")\n",
    "\n",
    "if example_task_path.is_file():\n",
    "    result = format_task_file_to_strings(example_task_path, train_template, test_template)\n",
    "    if result is not None:\n",
    "        train_str, test_str, test_answer = result\n",
    "        print(\"\\n--- TRAIN ---\\n\")\n",
    "        print(train_str)\n",
    "        print(\"\\n--- TEST ---\\n\")\n",
    "        print(test_str)\n",
    "        print(\"\\n--- TEST_ANSWER ---\\n\")\n",
    "        print(test_answer)\n",
    "    else:\n",
    "        print(\"An error occurred during formatting.\")\n",
    "else:\n",
    "    print(f\"Error: Example file not found at {example_task_path}. Please check your input folder.\")\n",
    "\n",
    "print(\"\\n--- End of Test ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main JSONL creation function defined.\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Define the Main Function to Create the JSONL File\n",
    "\n",
    "def create_formatted_jsonl_from_folder(input_folder, output_jsonl_path, train_template, test_template):\n",
    "    \"\"\"\n",
    "    Iterates through .json files in the input folder, formats each using the custom functions,\n",
    "    and writes the result to a JSON Lines file. Each line is a JSON object with keys:\n",
    "    \"train\", \"test\", and \"test_answer\".\n",
    "    \n",
    "    Args:\n",
    "        input_folder (str or Path): Directory containing the JSON task files.\n",
    "        output_jsonl_path (str or Path): Output filename for the JSONL file.\n",
    "        train_template (str): Template for training examples.\n",
    "        test_template (str): Template for test inputs.\n",
    "    \"\"\"\n",
    "    input_folder = Path(input_folder)\n",
    "    output_jsonl_path = Path(output_jsonl_path)\n",
    "    json_files = sorted(list(input_folder.glob(\"*.json\")))\n",
    "    num_files = len(json_files)\n",
    "\n",
    "    if num_files == 0:\n",
    "        print(\"Warning: No .json files found. Output file will be empty.\")\n",
    "        output_jsonl_path.touch()\n",
    "        return\n",
    "\n",
    "    print(f\"--- Starting Formatted JSONL Creation ---\")\n",
    "    print(f\"Reading from: {input_folder.resolve()}\")\n",
    "    print(f\"Writing to:   {output_jsonl_path.resolve()}\")\n",
    "    print(f\"Found {num_files} .json files to process.\")\n",
    "\n",
    "    lines_written = 0\n",
    "    files_skipped = 0\n",
    "    \n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as outfile:\n",
    "        for json_path in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "            result = format_task_file_to_strings(json_path, train_template, test_template)\n",
    "            if result is not None:\n",
    "                train_str, test_str, test_answer = result\n",
    "                json_obj = {\"train\": train_str, \"test\": test_str, \"test_answer\": test_answer}\n",
    "                outfile.write(json.dumps(json_obj, separators=(',', ':')) + \"\\n\")\n",
    "                lines_written += 1\n",
    "            else:\n",
    "                files_skipped += 1\n",
    "\n",
    "    print(\"--- Formatted JSONL Creation Complete ---\")\n",
    "    print(f\"Successfully wrote {lines_written} lines to {output_jsonl_path}\")\n",
    "    if files_skipped > 0:\n",
    "        print(f\"Skipped {files_skipped} file(s) due to processing errors.\")\n",
    "\n",
    "print(\"Main JSONL creation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory set to: C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\preprocessing\\modded_tasks\n",
      "Output file set to:    C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\preprocessing\\formatted_arc_tasks_custom.jsonl\n",
      "--- Starting Formatted JSONL Creation ---\n",
      "Reading from: C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\preprocessing\\modded_tasks\n",
      "Writing to:   C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\preprocessing\\formatted_arc_tasks_custom.jsonl\n",
      "Found 30138 .json files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 30138/30138 [00:47<00:00, 636.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Formatted JSONL Creation Complete ---\n",
      "Successfully wrote 30138 lines to formatted_arc_tasks_custom.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Configure Paths for the Conversion and Run the Main Conversion\n",
    "\n",
    "# Path to the folder with your .json task files\n",
    "input_folder_path = \"modded_tasks\"\n",
    "\n",
    "# Output JSONL filename (you can add a path like \"output/formatted_tasks.jsonl\" if needed)\n",
    "output_jsonl_filename = \"formatted_arc_tasks_custom.jsonl\"\n",
    "\n",
    "# Resolve paths\n",
    "input_folder = Path(input_folder_path)\n",
    "output_file = Path(output_jsonl_filename)\n",
    "\n",
    "print(f\"Input directory set to: {input_folder.resolve()}\")\n",
    "print(f\"Output file set to:    {output_file.resolve()}\")\n",
    "\n",
    "# Run the conversion over all JSON files in the folder\n",
    "create_formatted_jsonl_from_folder(input_folder, output_file, train_template, test_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First JSON object in the output file:\n",
      "{\n",
      "  \"train\": \"### Train Example 1:\\nInput:\\n[\\n[3,1],\\n[1,4],\\n]\\n\\nOutput:\\n[\\n[3,1,3,1,3,1],\\n[1,4,1,4,1,4],\\n[1,3,1,3,1,3],\\n[4,1,4,1,4,1],\\n[3,1,3,1,3,1],\\n[1,4,1,4,1,4],\\n]\\n\\n### Train Example 2:\\nInput:\\n[\\n[9,0],\\n[4,6],\\n]\\n\\nOutput:\\n[\\n[9,0,9,0,9,0],\\n[4,6,4,6,4,6],\\n[0,9,0,9,0,9],\\n[6,4,6,4,6,4],\\n[9,0,9,0,9,0],\\n[4,6,4,6,4,6],\\n]\",\n",
      "  \"test\": \"### Test Input:\\n[\\n[6,5],\\n[9,3],\\n]\",\n",
      "  \"test_answer\": [\n",
      "    [\n",
      "      6,\n",
      "      5,\n",
      "      6,\n",
      "      5,\n",
      "      6,\n",
      "      5\n",
      "    ],\n",
      "    [\n",
      "      9,\n",
      "      3,\n",
      "      9,\n",
      "      3,\n",
      "      9,\n",
      "      3\n",
      "    ],\n",
      "    [\n",
      "      5,\n",
      "      6,\n",
      "      5,\n",
      "      6,\n",
      "      5,\n",
      "      6\n",
      "    ],\n",
      "    [\n",
      "      3,\n",
      "      9,\n",
      "      3,\n",
      "      9,\n",
      "      3,\n",
      "      9\n",
      "    ],\n",
      "    [\n",
      "      6,\n",
      "      5,\n",
      "      6,\n",
      "      5,\n",
      "      6,\n",
      "      5\n",
      "    ],\n",
      "    [\n",
      "      9,\n",
      "      3,\n",
      "      9,\n",
      "      3,\n",
      "      9,\n",
      "      3\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ## 7. Verification (Optional)\n",
    "# To verify the conversion, we display the first JSON object in the output file.\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    try:\n",
    "        json_obj = json.loads(first_line)\n",
    "        print(\"First JSON object in the output file:\")\n",
    "        print(json.dumps(json_obj, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the output file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to Hugginface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\lukhausen\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lukhausen\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lukhausen\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c39e26b7c0e4dd9a9c20a518fe2c6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390a9ef0da244ab9a4eb1b68a8b14883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99ec7375f434f5c9a329222d28426a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/31 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58a46d4fa19496482b55827001fe0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/372 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Lukhausen/arc-agi-lepus-v1/commit/c223c28fb08beb76f77f01df3a529d6888443326', commit_message='Upload dataset', commit_description='', oid='c223c28fb08beb76f77f01df3a529d6888443326', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Lukhausen/arc-agi-lepus-v1', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Lukhausen/arc-agi-lepus-v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install datasets\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your JSONL file\n",
    "dataset = Dataset.from_json(\"formatted_arc_tasks_custom.jsonl\")\n",
    "\n",
    "# Upload to Hugging Face Hub\n",
    "dataset.push_to_hub(\"Lukhausen/arc-agi-lepus-v1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
