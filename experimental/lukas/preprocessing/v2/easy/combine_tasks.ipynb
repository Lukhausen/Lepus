{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ARC JSON Tasks to Custom JSONL Format\n",
    "\n",
    "This notebook converts a folder of ARC task `.json` files into a JSON Lines (`.jsonl`) file where each line is a JSON object with three fields:\n",
    "\n",
    "- **train**: A string with formatted training examples (inputs and outputs).\n",
    "- **test**: A string with formatted test inputs.\n",
    "- **test_answer**: The test answer(s) as the raw grid arrays (if only one test example is present, then the output grid is provided directly; if multiple, a list of grids is provided).\n",
    "\n",
    "The notebook is now reorganized into several helper functions:\n",
    "\n",
    "1. **get_train_string**: Returns the formatted training examples string.\n",
    "2. **get_test_string**: Returns the formatted test inputs string.\n",
    "3. **get_test_answer**: Returns the ground truth output(s) from test examples.\n",
    "4. **format_task_file_to_strings**: Returns a tuple of all three strings for one task file.\n",
    "\n",
    "Then the notebook tests the functions on a single task file and finally loops over all files in a folder to create the JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# ## 1. Setup: Import Libraries\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm  # for progress visualization\n",
    "import traceback  # for improved error reporting\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_grid_to_string(grid):\n",
    "    \"\"\"\n",
    "    Converts a grid (list of lists) into a nested list string format.\n",
    "    Each row becomes a list, and line breaks appear only after each inner list.\n",
    "\n",
    "    Args:\n",
    "        grid (list[list[int]]): The input grid.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted nested list as a string with minimal line breaks.\n",
    "    \"\"\"\n",
    "    if not grid or not isinstance(grid, list) or not isinstance(grid[0], list):\n",
    "        return \"\"\n",
    "    inner_lists = [f\"[{''.join(map(str, row))}]\" for row in grid]\n",
    "    return \"[\" + \",\\n\".join(inner_lists) + \"]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom formatting functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ## 3. Define Formatting Functions for Each Component\n",
    "\n",
    "def get_train_string(task_data, train_template):\n",
    "    \"\"\"\n",
    "    Formats the training examples using the provided template.\n",
    "    \n",
    "    Args:\n",
    "        task_data (dict): The loaded JSON data for a task.\n",
    "        train_template (str): Template for training examples. Expected placeholders: {index}, {input}, {output}.\n",
    "    \n",
    "    Returns:\n",
    "        str: The formatted training examples.\n",
    "    \"\"\"\n",
    "    train_examples = task_data.get(\"train\", [])\n",
    "    formatted_examples = []\n",
    "    for idx, example in enumerate(train_examples):\n",
    "        input_str = format_grid_to_string(example.get(\"input\"))\n",
    "        output_str = format_grid_to_string(example.get(\"output\"))\n",
    "        formatted_examples.append(\n",
    "            train_template.format(index=idx + 1, input=input_str, output=output_str)\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted_examples)\n",
    "\n",
    "\n",
    "def get_test_string(task_data, test_template):\n",
    "    \"\"\"\n",
    "    Formats the test inputs using the provided template.\n",
    "    \n",
    "    Args:\n",
    "        task_data (dict): The loaded JSON data for a task.\n",
    "        test_template (str): Template for test examples. Expected placeholders: {index}, {input}.\n",
    "    \n",
    "    Returns:\n",
    "        str: The formatted test input examples.\n",
    "    \"\"\"\n",
    "    test_examples = task_data.get(\"test\", [])\n",
    "    formatted_examples = []\n",
    "    for idx, example in enumerate(test_examples):\n",
    "        input_str = format_grid_to_string(example.get(\"input\"))\n",
    "        formatted_examples.append(\n",
    "            test_template.format(index=idx + 1, input=input_str)\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted_examples)\n",
    "\n",
    "\n",
    "def get_test_answer(task_data):\n",
    "    \"\"\"\n",
    "    Extracts the ground truth output(s) from the test examples.\n",
    "    If there is only one test example with an output, returns it directly; otherwise returns a list of outputs.\n",
    "    \n",
    "    Args:\n",
    "        task_data (dict): The loaded JSON data for a task.\n",
    "    \n",
    "    Returns:\n",
    "        object: The formatted test output or list of outputs.\n",
    "    \"\"\"\n",
    "    test_examples = task_data.get(\"test\", [])\n",
    "    answers = []\n",
    "    for example in test_examples:\n",
    "        answers.append(example.get(\"output\", []))\n",
    "    return answers[0] if len(answers) == 1 else answers\n",
    "\n",
    "\n",
    "def format_task_file_to_strings(json_path, train_template, test_template):\n",
    "    \"\"\"\n",
    "    Loads a JSON task file and returns a tuple with the three formatted components:\n",
    "    (train_string, test_string, test_answer).\n",
    "    \n",
    "    Args:\n",
    "        json_path (str or Path): Path to the JSON file.\n",
    "        train_template (str): Template for formatting training examples.\n",
    "        test_template (str): Template for formatting test inputs.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_string, test_string, test_answer) or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_path = Path(json_path)\n",
    "        with open(json_path, 'r', encoding='utf-8') as infile:\n",
    "            task_data = json.load(infile)\n",
    "        \n",
    "        train_str = get_train_string(task_data, train_template)\n",
    "        test_str = get_test_string(task_data, test_template)\n",
    "        test_answer = get_test_answer(task_data)\n",
    "        \n",
    "        return train_str, test_str, test_answer\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing file {json_path.name}: {e}\")\n",
    "        # Uncomment below to see the full traceback if needed\n",
    "        # traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Custom formatting functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing custom formatting for: 0a1d4ef5_r90_mh_c.json ---\n",
      "\n",
      "--- TRAIN ---\n",
      "\n",
      "### Train Example 1:\n",
      "Input:\n",
      "[[343443364343444366443663344433],\n",
      "[443343446334343444434344444444],\n",
      "[443634643443344646344664436434],\n",
      "[444439993643433442222444434664],\n",
      "[664449994611144362222643333444],\n",
      "[333469993411166642222436644366],\n",
      "[343449996411163462222643633646],\n",
      "[446369993633664442222643344344],\n",
      "[343449994464443446344363344343],\n",
      "[634443443434366466434633434344],\n",
      "[333344444443463643343634334366],\n",
      "[444364646444636646434664443434],\n",
      "[638888843555364369999366444646],\n",
      "[448888844555364349999433643644],\n",
      "[448888844555443349999444443443],\n",
      "[338888864555444649999363443443],\n",
      "[438888866344434434344433344463],\n",
      "[338888846334344346666443444444],\n",
      "[433463463346634644634346443434],\n",
      "[443644444464344366344443443433],\n",
      "[464434343344344888883336464644],\n",
      "[464434633444366888883344466434],\n",
      "[343999944111146888884446466336],\n",
      "[644999934111146888884443434434],\n",
      "[433999964111146888883443433443],\n",
      "[334999934111144644343343634444],\n",
      "[334343344111166434433443443446],\n",
      "[634434446434446344633344643636],\n",
      "[663366364333463444634344344446],\n",
      "[646444433464644443434434343343]]\n",
      "\n",
      "Output:\n",
      "[[912],\n",
      "[859],\n",
      "[918]]\n",
      "\n",
      "### Train Example 2:\n",
      "Input:\n",
      "[[449929444949444424492442944492],\n",
      "[244494494444224494442944294424],\n",
      "[442944494429492449244924442449],\n",
      "[411111994477777774444494429429],\n",
      "[411111444477777772944424422994],\n",
      "[211111444477777774444944999494],\n",
      "[411111924977777774492924424444],\n",
      "[911111444244994444499444994449],\n",
      "[411111494999444424224949444944],\n",
      "[444294242244294244929499924424],\n",
      "[444294942949242949494944224249],\n",
      "[242242242488888444944442444944],\n",
      "[433332424488888944492444444449],\n",
      "[233332494488888924422429444494],\n",
      "[433339292488888292429444929494],\n",
      "[233339449488888949994494424444],\n",
      "[433339449488888444444444424499],\n",
      "[433339442222944944942444922444],\n",
      "[233334494944494424944444424442],\n",
      "[933332444242929494944424222444],\n",
      "[424949422442424444494292442494],\n",
      "[494442944492449444942292444429],\n",
      "[294924424444429424422444944294],\n",
      "[426666642443333344442494499999],\n",
      "[946666694423333349994442244244],\n",
      "[446666644943333342444424292924],\n",
      "[446666649443333344244944944492],\n",
      "[446666692443333344242249444429],\n",
      "[444944999429449444492449499924],\n",
      "[444944944444294242229494429424]]\n",
      "\n",
      "Output:\n",
      "[[17],\n",
      "[38],\n",
      "[63]]\n",
      "\n",
      "### Train Example 3:\n",
      "Input:\n",
      "[[400404444444424444422242444420],\n",
      "[244044442224404442420404424424],\n",
      "[442404424424044202240424044424],\n",
      "[402040222222222424444404420242],\n",
      "[429999004433334449999902440442],\n",
      "[229999004433334049999920404440],\n",
      "[009999044233330429999924422442],\n",
      "[229999244433334249999944202422],\n",
      "[209999222433332409999920442444],\n",
      "[449999444424022244204440004404],\n",
      "[049999402220242442240442024404],\n",
      "[220222444024444044044424442444],\n",
      "[024424202211111040440002244442],\n",
      "[002444044411111408888242042240],\n",
      "[408888440411111448888004444204],\n",
      "[228888042211111248888444444442],\n",
      "[228888224411111448888440440224],\n",
      "[008888424411111028888200202240],\n",
      "[240424244211111222444044424404],\n",
      "[204424440442424244240444222444],\n",
      "[024242022244204244004244402440],\n",
      "[042422444455555242402240424440],\n",
      "[441111040455555444333334044000],\n",
      "[241111242455555004333332444244],\n",
      "[401111424255555200333334444224],\n",
      "[441111244455555444333330444242],\n",
      "[201111420042442424333334444424],\n",
      "[404404042440244244204444402424],\n",
      "[444244044000042242444442404424],\n",
      "[422402244440242404244440044000]]\n",
      "\n",
      "Output:\n",
      "[[939],\n",
      "[818],\n",
      "[153]]\n",
      "\n",
      "--- TEST ---\n",
      "\n",
      "### Test Input:\n",
      "[[094449904499444944444044940449],\n",
      "[090409044440990994404409440044],\n",
      "[400444494094409440440044444940],\n",
      "[400004490488888400494900444044],\n",
      "[444022224088888444444400444494],\n",
      "[990022229088888440409488880944],\n",
      "[049922224488888943333988880044],\n",
      "[444922220988888403333488889004],\n",
      "[400422220404444903333004000090],\n",
      "[999422224944444493333449409499],\n",
      "[944444494904449404004400944999],\n",
      "[944004444494444404944444494909],\n",
      "[404994944455559411110477777444],\n",
      "[990111114055554411119477777444],\n",
      "[444111114955554411114077777040],\n",
      "[490111110455554011114977777400],\n",
      "[049111110455554411110477777940],\n",
      "[099111110444044411119477777000],\n",
      "[440090444444490011119044409049],\n",
      "[099499444090444094904944440440],\n",
      "[440994444404094949044422229444],\n",
      "[044411144444944409409422224404],\n",
      "[900911104333330049944022224040],\n",
      "[444911104333334455555422229999],\n",
      "[494411144333334055555922220440],\n",
      "[494911144333334955555922224409],\n",
      "[444911144333330455555940449400],\n",
      "[444444049044004409409444440440],\n",
      "[440449449000000444044409494944],\n",
      "[940004404044449009044000444090]]\n",
      "\n",
      "--- TEST_ANSWER ---\n",
      "\n",
      "[[2, 8, 3, 8], [1, 5, 1, 7], [1, 3, 5, 2]]\n",
      "\n",
      "--- End of Test ---\n"
     ]
    }
   ],
   "source": [
    "# ## 4. Test the Function on a Single Task File\n",
    "\n",
    "# Define custom templates for train and test inputs\n",
    "train_template = (\n",
    "    \"### Train Example {index}:\\n\"\n",
    "    \"Input:\\n\"\n",
    "    \"{input}\\n\\n\"\n",
    "    \"Output:\\n\"\n",
    "    \"{output}\"\n",
    ")\n",
    "\n",
    "test_template = (\n",
    "    \"### Test Input:\\n\"\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "# Set the sample task filename (adjust the folder/filename as needed)\n",
    "test_input_folder = \"modded_tasks\"  # Folder containing your ARC JSON task files\n",
    "example_task_filename = \"0a1d4ef5_r90_mh_c.json\"  # Example file name\n",
    "\n",
    "example_task_path = Path(test_input_folder) / example_task_filename\n",
    "\n",
    "print(f\"--- Testing custom formatting for: {example_task_path.name} ---\")\n",
    "\n",
    "if example_task_path.is_file():\n",
    "    result = format_task_file_to_strings(example_task_path, train_template, test_template)\n",
    "    if result is not None:\n",
    "        train_str, test_str, test_answer = result\n",
    "        print(\"\\n--- TRAIN ---\\n\")\n",
    "        print(train_str)\n",
    "        print(\"\\n--- TEST ---\\n\")\n",
    "        print(test_str)\n",
    "        print(\"\\n--- TEST_ANSWER ---\\n\")\n",
    "        print(test_answer)\n",
    "    else:\n",
    "        print(\"An error occurred during formatting.\")\n",
    "else:\n",
    "    print(f\"Error: Example file not found at {example_task_path}. Please check your input folder.\")\n",
    "\n",
    "print(\"\\n--- End of Test ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main JSONL creation function defined.\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Define the Main Function to Create the JSONL File\n",
    "\n",
    "def create_formatted_jsonl_from_folder(input_folder, output_jsonl_path, train_template, test_template):\n",
    "    \"\"\"\n",
    "    Iterates through .json files in the input folder, formats each using the custom functions,\n",
    "    and writes the result to a JSON Lines file. Each line is a JSON object with keys:\n",
    "    \"train\", \"test\", and \"test_answer\".\n",
    "    \n",
    "    Args:\n",
    "        input_folder (str or Path): Directory containing the JSON task files.\n",
    "        output_jsonl_path (str or Path): Output filename for the JSONL file.\n",
    "        train_template (str): Template for training examples.\n",
    "        test_template (str): Template for test inputs.\n",
    "    \"\"\"\n",
    "    input_folder = Path(input_folder)\n",
    "    output_jsonl_path = Path(output_jsonl_path)\n",
    "    json_files = sorted(list(input_folder.glob(\"*.json\")))\n",
    "    num_files = len(json_files)\n",
    "\n",
    "    if num_files == 0:\n",
    "        print(\"Warning: No .json files found. Output file will be empty.\")\n",
    "        output_jsonl_path.touch()\n",
    "        return\n",
    "\n",
    "    print(f\"--- Starting Formatted JSONL Creation ---\")\n",
    "    print(f\"Reading from: {input_folder.resolve()}\")\n",
    "    print(f\"Writing to:   {output_jsonl_path.resolve()}\")\n",
    "    print(f\"Found {num_files} .json files to process.\")\n",
    "\n",
    "    lines_written = 0\n",
    "    files_skipped = 0\n",
    "    \n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as outfile:\n",
    "        for json_path in tqdm(json_files, desc=\"Processing JSON files\"):\n",
    "            result = format_task_file_to_strings(json_path, train_template, test_template)\n",
    "            if result is not None:\n",
    "                train_str, test_str, test_answer = result\n",
    "                json_obj = {\"train\": train_str, \"test\": test_str, \"test_answer\": test_answer}\n",
    "                outfile.write(json.dumps(json_obj, separators=(',', ':')) + \"\\n\")\n",
    "                lines_written += 1\n",
    "            else:\n",
    "                files_skipped += 1\n",
    "\n",
    "    print(\"--- Formatted JSONL Creation Complete ---\")\n",
    "    print(f\"Successfully wrote {lines_written} lines to {output_jsonl_path}\")\n",
    "    if files_skipped > 0:\n",
    "        print(f\"Skipped {files_skipped} file(s) due to processing errors.\")\n",
    "\n",
    "print(\"Main JSONL creation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory set to: C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\preprocessing\\v2\\easy\\modded_tasks\n",
      "Output file set to:    C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\preprocessing\\v2\\easy\\formatted_arc_tasks_easy.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Formatted JSONL Creation ---\n",
      "Reading from: C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\preprocessing\\v2\\easy\\modded_tasks\n",
      "Writing to:   C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\preprocessing\\v2\\easy\\formatted_arc_tasks_easy.jsonl\n",
      "Found 28158 .json files to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 28158/28158 [00:19<00:00, 1470.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Formatted JSONL Creation Complete ---\n",
      "Successfully wrote 28158 lines to formatted_arc_tasks_easy.jsonl\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Configure Paths for the Conversion and Run the Main Conversion\n",
    "\n",
    "# Path to the folder with your .json task files\n",
    "input_folder_path = \"modded_tasks\"\n",
    "\n",
    "# Output JSONL filename (you can add a path like \"output/formatted_tasks.jsonl\" if needed)\n",
    "output_jsonl_filename = \"formatted_arc_tasks_easy.jsonl\"\n",
    "\n",
    "# Resolve paths\n",
    "input_folder = Path(input_folder_path)\n",
    "output_file = Path(output_jsonl_filename)\n",
    "\n",
    "print(f\"Input directory set to: {input_folder.resolve()}\")\n",
    "print(f\"Output file set to:    {output_file.resolve()}\")\n",
    "\n",
    "# Run the conversion over all JSON files in the folder\n",
    "create_formatted_jsonl_from_folder(input_folder, output_file, train_template, test_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First JSON object in the output file:\n",
      "{\n",
      "  \"train\": \"### Train Example 1:\\nInput:\\n[[60],\\n[08]]\\n\\nOutput:\\n[[606060],\\n[080808],\\n[060606],\\n[808080],\\n[606060],\\n[080808]]\\n\\n### Train Example 2:\\nInput:\\n[[25],\\n[87]]\\n\\nOutput:\\n[[252525],\\n[878787],\\n[525252],\\n[787878],\\n[252525],\\n[878787]]\",\n",
      "  \"test\": \"### Test Input:\\n[[71],\\n[26]]\",\n",
      "  \"test_answer\": [\n",
      "    [\n",
      "      7,\n",
      "      1,\n",
      "      7,\n",
      "      1,\n",
      "      7,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      6,\n",
      "      2,\n",
      "      6,\n",
      "      2,\n",
      "      6\n",
      "    ],\n",
      "    [\n",
      "      1,\n",
      "      7,\n",
      "      1,\n",
      "      7,\n",
      "      1,\n",
      "      7\n",
      "    ],\n",
      "    [\n",
      "      6,\n",
      "      2,\n",
      "      6,\n",
      "      2,\n",
      "      6,\n",
      "      2\n",
      "    ],\n",
      "    [\n",
      "      7,\n",
      "      1,\n",
      "      7,\n",
      "      1,\n",
      "      7,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      2,\n",
      "      6,\n",
      "      2,\n",
      "      6,\n",
      "      2,\n",
      "      6\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ## 7. Verification (Optional)\n",
    "# To verify the conversion, we display the first JSON object in the output file.\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    try:\n",
    "        json_obj = json.loads(first_line)\n",
    "        print(\"First JSON object in the output file:\")\n",
    "        print(json.dumps(json_obj, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the output file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload to Hugginface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\lukhausen\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lukhausen\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lukhausen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lukhausen\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f491a384d6e34a5b875e2cd3bdf26465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2408d029316944f3b7551f316713ceae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a08770999a4c9db69f9cbf843717cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/29 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Lukhausen/arc-agi-lepus-v1-easy/commit/e47a415a244de9034a09d572287f79623bdb7d65', commit_message='Upload dataset', commit_description='', oid='e47a415a244de9034a09d572287f79623bdb7d65', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Lukhausen/arc-agi-lepus-v1-easy', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Lukhausen/arc-agi-lepus-v1-easy'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your JSONL file\n",
    "dataset = Dataset.from_json(\"formatted_arc_tasks_easy.jsonl\")\n",
    "\n",
    "# Upload to Hugging Face Hub\n",
    "dataset.push_to_hub(\"Lukhausen/arc-agi-lepus-v1-easy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
