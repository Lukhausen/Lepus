{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interacting with the ARC Dataset and LLMs\n",
        "\n",
        "This notebook provides a foundational workflow for exploring the Abstraction and Reasoning Corpus (ARC) dataset using Large Language Models (LLMs).\n",
        "\n",
        "**Understanding ARC:**\n",
        "\n",
        "The ARC dataset, created by FranÃ§ois Chollet, is designed to test abstract reasoning and intelligence, moving beyond simple pattern recognition. Each ARC task challenges a system to infer an underlying transformation rule from a few examples and then apply that rule to new, unseen inputs.\n",
        "\n",
        "**Task Structure:**\n",
        "\n",
        "Every ARC task consists of:\n",
        "*   **`train` pairs:** A small set (usually 2-5) of input/output grid examples. The goal is to *learn* the transformation rule by observing how the input grids change to become the output grids in these examples.\n",
        "*   **`test` pairs:** One or more input grids (and their corresponding *unseen* solution output grids). After inferring the rule from the `train` set, the system must apply it to the `test` input grids to generate the correct `test` output grids.\n",
        "\n",
        "**The Challenge:** The core challenge is the *abstraction* of the rule from the `train` examples and its *generalization* to the `test` inputs.\n",
        "\n",
        "**Evaluation Rule (ARC Prize):**\n",
        "\n",
        "For a task to be considered solved:\n",
        "1.  The system must generate the correct output grid for **every single `test` input grid** within that task.\n",
        "2.  Each prediction must be an *exact* match to the ground truth solution grid.\n",
        "3.  If a task has multiple `test` inputs, **all** of them must be solved correctly based on the single rule inferred from the `train` set.\n",
        "\n",
        "**This Notebook's Goal (Initial Steps):**\n",
        "\n",
        "This part focuses on the initial setup and data handling:\n",
        "1.  Setting up the environment (installing and importing libraries).\n",
        "2.  Performing a basic API call test (assuming credentials are set).\n",
        "3.  Loading ARC task data from JSON files.\n",
        "4.  Understanding the structure of the loaded data.\n",
        "5.  Providing helper functions to easily access specific parts of a task (train pairs, test inputs, test outputs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Libraries and API Test\n",
        "\n",
        "First, we install and import the necessary libraries. We assume you have Python and pip installed.\n",
        "*   `python-dotenv`: To potentially load API keys from a `.env` file (though we won't explicitly check for the key here).\n",
        "*   `litellm`: To interact with LLM APIs.\n",
        "*   `numpy`: For potential numerical operations later (though not strictly needed for just loading/parsing).\n",
        "\n",
        "We'll also perform a minimal API call to ensure `litellm` is configured correctly and can reach the service. **Note:** This step assumes your API key (e.g., `OPENAI_API_KEY`) is already set as an environment variable or globally configured for `litellm`. If not set, this test call will fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking/Installing required libraries...\n",
            "Libraries found.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if they aren't already installed\n",
        "print(\"Checking/Installing required libraries...\")\n",
        "try:\n",
        "    import litellm\n",
        "    import dotenv\n",
        "    import numpy\n",
        "    print(\"Libraries found.\")\n",
        "except ImportError:\n",
        "    print(\"Installing python-dotenv, litellm, numpy...\")\n",
        "    %pip install -q python-dotenv litellm numpy\n",
        "    print(\"Installation complete. You might need to restart the kernel.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "\n",
        "# Import numpy just in case it's needed later, though not strictly for loading\n",
        "import numpy as np \n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import litellm\n",
        "from litellm import completion\n",
        "\n",
        "# Attempt to load environment variables from .env file (optional)\n",
        "load_dotenv()\n",
        "\n",
        "# Reduce LiteLLM's default logging verbosity for a cleaner output\n",
        "litellm.set_verbose = False\n",
        "\n",
        "print(\"Libraries imported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing minimal API test call...\n",
            "API test call successful (received a response).\n"
          ]
        }
      ],
      "source": [
        "# --- Minimal API Key Test --- \n",
        "# This performs a very small API call to check basic connectivity.\n",
        "# It ASSUMES your API key is correctly set in your environment.\n",
        "# If this cell fails, check your API key setup (e.g., OPENAI_API_KEY env var).\n",
        "print(\"Performing minimal API test call...\")\n",
        "try:\n",
        "    response = completion(\n",
        "        model=\"gpt-4o\", # Or \"gpt-3.5-turbo\" or another model you have access to\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Respond with just 'OK'.\"}],\n",
        "        max_tokens=2,        # Limit response length\n",
        "        request_timeout=20 # Short timeout\n",
        "    )\n",
        "    # We don't strictly need to check the content, just that it didn't crash\n",
        "    print(\"API test call successful (received a response).\") \n",
        "    # print(f\"Test Response: {response.choices[0].message.content}\") # Optional: view response\n",
        "except Exception as e:\n",
        "    print(f\"\\033[91mAPI test call failed: {e}\\033[0m\")\n",
        "    print(\"Please ensure your API key (e.g., OPENAI_API_KEY) is set correctly as an environment variable or in a .env file,\")\n",
        "    print(\"and that you have access to the specified model ('gpt-4o' used here).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "This function loads ARC tasks from a specified directory containing `.json` files. It assumes the files are correctly formatted ARC tasks. We are skipping complex error checking for this developer-focused setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Attempting to load data from: C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\ARC-AGI\\data\\evaluation\n",
            "Limiting loading to the first 20 tasks.\n",
            "Processing 20 task files from C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\ARC-AGI\\data\\evaluation...\n",
            "\n",
            "Successfully attempted to load 20 tasks.\n",
            "\n",
            "Example Task IDs loaded: ['0934a4d8', '135a2760', '136b0064', '13e47133', '142ca369', '16b78196', '16de56c4', '1818057f', '195c6913', '1ae2feb7']\n"
          ]
        }
      ],
      "source": [
        "# --- Type Definitions --- \n",
        "# Define types for clarity, even in a simplified script\n",
        "Grid = List[List[int]] # Represents a 2D grid of integers\n",
        "TaskPair = Dict[str, Grid] # Represents one {'input': Grid, 'output': Grid} pair\n",
        "TaskData = Dict[str, List[TaskPair]] # Represents the core content {'train': List[TaskPair], 'test': List[TaskPair]}\n",
        "\n",
        "def load_arc_tasks_simple(data_dir: str, limit: Optional[int] = None) -> Dict[str, TaskData]:\n",
        "    \"\"\"Loads ARC tasks from JSON files into a dictionary (simplified error handling).\"\"\"\n",
        "    arc_path = Path(data_dir)\n",
        "    if not arc_path.is_dir():\n",
        "        print(f\"Error: ARC data directory not found: {arc_path.resolve()}\")\n",
        "        # In a truly simple script, we might just let it raise the error later,\n",
        "        # but checking the dir is a minimal useful check.\n",
        "        return {}\n",
        "\n",
        "    # Assume directory scanning works\n",
        "    json_files = sorted(list(arc_path.glob(\"*.json\")))\n",
        "\n",
        "    if not json_files:\n",
        "        print(f\"Warning: No JSON files found in {arc_path.resolve()}\")\n",
        "        return {}\n",
        "\n",
        "    if limit:\n",
        "        json_files = json_files[:limit]\n",
        "        print(f\"Limiting loading to the first {len(json_files)} tasks.\")\n",
        "\n",
        "    loaded_tasks: Dict[str, TaskData] = {}\n",
        "    print(f\"Processing {len(json_files)} task files from {arc_path.resolve()}...\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        task_id = json_file.stem\n",
        "        # Assume file is readable and valid JSON with 'train' and 'test' keys\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            task_data = json.load(f)\n",
        "        loaded_tasks[task_id] = task_data # Store the raw loaded data\n",
        "\n",
        "    print(f\"\\nSuccessfully attempted to load {len(loaded_tasks)} tasks.\")\n",
        "    return loaded_tasks\n",
        "\n",
        "# --- Load the Data --- \n",
        "# <<< IMPORTANT: UPDATE THIS PATH TO YOUR ARC DATA DIRECTORY >>>\n",
        "ARC_DATA_DIR = \"../ARC-AGI/data/evaluation\" # Adjust this path!\n",
        "TASK_LOAD_LIMIT = 20 # Load only a few tasks for faster testing\n",
        "\n",
        "print(f\"\\nAttempting to load data from: {Path(ARC_DATA_DIR).resolve()}\")\n",
        "all_task_data: Dict[str, TaskData] = load_arc_tasks_simple(ARC_DATA_DIR, limit=TASK_LOAD_LIMIT)\n",
        "\n",
        "if not all_task_data:\n",
        "    print(\"\\n--- CRITICAL: No tasks were loaded. Check ARC_DATA_DIR path. --- \")\n",
        "else:\n",
        "    print(f\"\\nExample Task IDs loaded: {list(all_task_data.keys())[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding ARC Task Data Structure\n",
        "\n",
        "The `all_task_data` variable loaded above is a Python dictionary.\n",
        "\n",
        "*   **Keys:** Each key is a string representing the `task_id` (e.g., `'0934a4d8'`).\n",
        "*   **Values:** Each value is another dictionary (`TaskData`) containing the data for that specific task, loaded directly from the corresponding JSON file.\n",
        "\n",
        "Inside each `TaskData` dictionary, the crucial keys are:\n",
        "\n",
        "1.  **`'train'`**: This holds a *list* of training examples.\n",
        "    *   Each element in the `'train'` list is a dictionary (`TaskPair`).\n",
        "    *   Each `TaskPair` dictionary has two keys:\n",
        "        *   `'input'`: Contains the input grid (`Grid`), which is a list of lists of integers (e.g., `[[0, 1], [2, 0]]`).\n",
        "        *   `'output'`: Contains the corresponding output grid (`Grid`) after the transformation rule has been applied.\n",
        "\n",
        "2.  **`'test'`**: This holds a *list* of test problems.\n",
        "    *   Each element in the `'test'` list is also a `TaskPair` dictionary.\n",
        "    *   Each `TaskPair` in the test list contains:\n",
        "        *   `'input'`: The test input grid (`Grid`) to which the learned rule must be applied.\n",
        "        *   `'output'`: The *ground truth* solution grid (`Grid`). The LLM does **not** see this during prediction; it's used for evaluation later.\n",
        "\n",
        "**LLM Interaction Strategy:**\n",
        "\n",
        "To ask an LLM to solve *one* test case for a given task, we need to provide:\n",
        "1.  All the `train` input/output pairs for that task.\n",
        "2.  The specific `test` input grid we want it to solve.\n",
        "\n",
        "Since a task can have multiple test cases in its `'test'` list, we typically need to make a separate LLM call for *each* test input grid within that task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helper Functions for Accessing Task Components\n",
        "\n",
        "To make it easier to prepare data for the LLM, let's create simple functions to extract the relevant pieces from the loaded `TaskData` for a specific task ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined: get_train_pairs, get_test_inputs, get_test_outputs\n"
          ]
        }
      ],
      "source": [
        "def get_train_pairs(task_id: str, all_tasks: Dict[str, TaskData]) -> List[TaskPair]:\n",
        "    \"\"\"Returns the list of training pairs for a given task ID.\"\"\"\n",
        "    # Assumes task_id exists and has 'train' key with a list\n",
        "    return all_tasks[task_id]['train']\n",
        "\n",
        "def get_test_inputs(task_id: str, all_tasks: Dict[str, TaskData]) -> List[Grid]:\n",
        "    \"\"\"Returns a list of all test input grids for a given task ID.\"\"\"\n",
        "    # Assumes task_id exists and has 'test' key with a list of pairs, each having 'input'\n",
        "    test_pairs = all_tasks[task_id]['test']\n",
        "    return [pair['input'] for pair in test_pairs]\n",
        "\n",
        "def get_test_outputs(task_id: str, all_tasks: Dict[str, TaskData]) -> List[Grid]:\n",
        "    \"\"\"Returns a list of all test output (solution) grids for a given task ID.\"\"\"\n",
        "    # Assumes task_id exists and has 'test' key with a list of pairs, each having 'output'\n",
        "    test_pairs = all_tasks[task_id]['test']\n",
        "    return [pair['output'] for pair in test_pairs]\n",
        "\n",
        "print(\"Helper functions defined: get_train_pairs, get_test_inputs, get_test_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Usage of Helper Functions\n",
        "\n",
        "Let's see how to use these functions to get the data components for the first task we loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Example data extraction for Task ID: 0934a4d8 ---\n",
            "Number of training pairs: 4\n",
            "Structure of first train pair: dict_keys(['input', 'output'])\n",
            "\n",
            "Number of test inputs: 1\n",
            "Type of first test input: <class 'list'>\n",
            "\n",
            "Number of test outputs (solutions): 1\n",
            "Type of first test output: <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "if all_task_data:\n",
        "    example_task_id = list(all_task_data.keys())[0] # Get the first loaded task ID\n",
        "    print(f\"--- Example data extraction for Task ID: {example_task_id} ---\")\n",
        "\n",
        "    # Get training pairs\n",
        "    train_pairs = get_train_pairs(example_task_id, all_task_data)\n",
        "    print(f\"Number of training pairs: {len(train_pairs)}\")\n",
        "    if train_pairs:\n",
        "        # Print the structure of the first training pair (input/output keys)\n",
        "        print(f\"Structure of first train pair: {train_pairs[0].keys()}\") \n",
        "        # print(f\"First train input grid: {train_pairs[0]['input']}\") # Optional: print grid\n",
        "\n",
        "    # Get test inputs\n",
        "    test_inputs = get_test_inputs(example_task_id, all_task_data)\n",
        "    print(f\"\\nNumber of test inputs: {len(test_inputs)}\")\n",
        "    if test_inputs:\n",
        "        print(f\"Type of first test input: {type(test_inputs[0])}\")\n",
        "        # print(f\"First test input grid: {test_inputs[0]}\") # Optional: print grid\n",
        "\n",
        "    # Get test outputs (solutions)\n",
        "    test_outputs = get_test_outputs(example_task_id, all_task_data)\n",
        "    print(f\"\\nNumber of test outputs (solutions): {len(test_outputs)}\")\n",
        "    if test_outputs:\n",
        "        print(f\"Type of first test output: {type(test_outputs[0])}\")\n",
        "        # print(f\"First test output grid: {test_outputs[0]}\") # Optional: print grid\n",
        "else:\n",
        "    print(\"\\nNo task data loaded, skipping example.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Prompt Strategy Templates\n",
        "\n",
        "Instead of complex functions, we can define our prompting strategy using simple string templates with placeholders. We'll use f-string style placeholders like `{placeholder_name}`.\n",
        "\n",
        "We need placeholders for:\n",
        "*   `{train_examples_string}`: Where we will insert the formatted string of all training input/output pairs.\n",
        "*   `{test_input_string}`: Where we will insert the formatted string of the single test input grid we want the LLM to solve.\n",
        "\n",
        "Below are example templates. You can modify these strings to experiment with different instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt templates defined (SYSTEM_PROMPT_TEMPLATE, USER_PROMPT_TEMPLATE).\n"
          ]
        }
      ],
      "source": [
        "# Define simple string templates for the prompts\n",
        "SYSTEM_PROMPT_TEMPLATE = (\n",
        "    \"You are an ARC puzzle solver. Analyze the train examples (input/output pairs). \"\n",
        "    \"Apply the deduced rule to the test input grid. \"\n",
        "    \"Output your reasoning and then a JSON array in a Codeblock for the predicted test output grid.\"\n",
        ")\n",
        "\n",
        "USER_PROMPT_TEMPLATE = (\n",
        "    \"**TRAIN EXAMPLES:**\\n\"\n",
        "    \"{train_examples_string}\\n\\n\"\n",
        "    \"**TEST INPUT GRID:**\\n\"\n",
        "    \"{test_input_string}\\n\\n\"\n",
        "    \"**PREDICTED OUTPUT GRID:**\"\n",
        ")\n",
        "\n",
        "print(\"Prompt templates defined (SYSTEM_PROMPT_TEMPLATE, USER_PROMPT_TEMPLATE).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Formatting Helpers and Prompt Preparation\n",
        "\n",
        "We need helper functions to format the ARC grid data into strings, and then a function to insert these strings into our templates to create the final messages for the LLM API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatting and prompt preparation functions defined.\n"
          ]
        }
      ],
      "source": [
        "# --- Formatting Helpers (Simplified) --- \n",
        "# Reuse or redefine the formatting helpers if needed\n",
        "\n",
        "def format_grid_to_string(grid: Grid) -> str:\n",
        "    \"\"\"Converts a grid (list of lists) to a compact JSON string representation.\"\"\"\n",
        "    # Assume grid is a list of lists of ints\n",
        "    return json.dumps(grid, separators=(',', ':'))\n",
        "\n",
        "def format_train_pairs_to_string(train_pairs: List[TaskPair]) -> str:\n",
        "    \"\"\"Formats a list of training pairs into a single string for the prompt.\"\"\"\n",
        "    formatted_pairs = []\n",
        "    for i, pair in enumerate(train_pairs):\n",
        "        input_str = format_grid_to_string(pair['input'])\n",
        "        output_str = format_grid_to_string(pair['output'])\n",
        "        formatted_pairs.append(f\"Example {i+1} Input:\\n{input_str}\\nExample {i+1} Output:\\n{output_str}\")\n",
        "    return \"\\n\\n\".join(formatted_pairs)\n",
        "\n",
        "# --- Prompt Preparation Function --- \n",
        "\n",
        "def prepare_prompt_messages(\n",
        "    system_template: str,\n",
        "    user_template: str,\n",
        "    train_pairs: List[TaskPair],\n",
        "    test_input_grid: Grid\n",
        ") -> List[Dict[str, str]]:\n",
        "    \"\"\"Formats data and inserts it into prompt templates to create API messages.\"\"\"\n",
        "\n",
        "    # Format the training examples and test input into strings\n",
        "    train_examples_str = format_train_pairs_to_string(train_pairs)\n",
        "    test_input_str = format_grid_to_string(test_input_grid)\n",
        "\n",
        "    # Use f-strings to fill the templates\n",
        "    # Note: The variable names inside the f-string (e.g., train_examples_str)\n",
        "    # must match the placeholders used in the template strings (e.g., {train_examples_string})\n",
        "    # We map our internal variable names to the placeholder names expected by the template\n",
        "    final_user_prompt = user_template.format(\n",
        "        train_examples_string=train_examples_str,\n",
        "        test_input_string=test_input_str\n",
        "    )\n",
        "    \n",
        "    # System prompt usually doesn't need formatting in this simple case\n",
        "    final_system_prompt = system_template \n",
        "\n",
        "    # Return the structured messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": final_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": final_user_prompt}\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "print(\"Formatting and prompt preparation functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Preparing messages using sample data ---\n",
            "\n",
            "Resulting 'messages' list (ready for LLM API):\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"You are an ARC puzzle solver. Analyze the train examples (input/output pairs). Apply the deduced rule to the test input grid. Output your reasoning and then a JSON array in a Codeblock for the predicted test output grid.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"**TRAIN EXAMPLES:**\\nExample 1 Input:\\n[[1,0],[0,0]]\\nExample 1 Output:\\n[[0,1],[0,0]]\\n\\nExample 2 Input:\\n[[0,0],[2,0]]\\nExample 2 Output:\\n[[0,0],[0,2]]\\n\\n**TEST INPUT GRID:**\\n[[0,3],[0,0]]\\n\\n**PREDICTED OUTPUT GRID:**\"\n",
            "  }\n",
            "]\n",
            "\n",
            "--- Content of the 'user' message: ---\n",
            "**TRAIN EXAMPLES:**\n",
            "Example 1 Input:\n",
            "[[1,0],[0,0]]\n",
            "Example 1 Output:\n",
            "[[0,1],[0,0]]\n",
            "\n",
            "Example 2 Input:\n",
            "[[0,0],[2,0]]\n",
            "Example 2 Output:\n",
            "[[0,0],[0,2]]\n",
            "\n",
            "**TEST INPUT GRID:**\n",
            "[[0,3],[0,0]]\n",
            "\n",
            "**PREDICTED OUTPUT GRID:**\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Sample Training Pairs (List of TaskPair dictionaries)\n",
        "sample_train_pairs: List[TaskPair] = [\n",
        "    {\n",
        "        'input': [[1, 0], [0, 0]],\n",
        "        'output': [[0, 1], [0, 0]]\n",
        "    },\n",
        "    {\n",
        "        'input': [[0, 0], [2, 0]],\n",
        "        'output': [[0, 0], [0, 2]]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Sample Test Input Grid (Grid)\n",
        "sample_test_input_grid: Grid = [[0, 3], [0, 0]]\n",
        "\n",
        "# --- Call the Preparation Function ---\n",
        "\n",
        "print(\"--- Preparing messages using sample data ---\")\n",
        "prepared_messages = prepare_prompt_messages(\n",
        "    system_template=SYSTEM_PROMPT_TEMPLATE,\n",
        "    user_template=USER_PROMPT_TEMPLATE,\n",
        "    train_pairs=sample_train_pairs,\n",
        "    test_input_grid=sample_test_input_grid\n",
        ")\n",
        "\n",
        "# --- Print the Result ---\n",
        "# Use json.dumps for pretty printing the list of dictionaries\n",
        "print(\"\\nResulting 'messages' list (ready for LLM API):\")\n",
        "print(json.dumps(prepared_messages, indent=2))\n",
        "\n",
        "# --- Optionally, print the formatted User prompt content for clarity ---\n",
        "print(\"\\n--- Content of the 'user' message: ---\")\n",
        "print(prepared_messages[1]['content'])\n",
        "print(\"--------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Get LLM Response (Simple)\n",
        "\n",
        "This function takes the prepared messages and sends them to the specified LLM using `litellm`. It returns the raw text response content, with minimal error handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM interaction function 'get_llm_response' defined.\n"
          ]
        }
      ],
      "source": [
        "def get_llm_response(messages: List[Dict[str, str]], model_name: str) -> Optional[str]:\n",
        "    \"\"\"Sends messages to the LLM and returns the raw response content.\"\"\"\n",
        "    print(f\"Sending request to model: {model_name}...\")\n",
        "    try:\n",
        "        response = completion(\n",
        "            model=model_name,\n",
        "            messages=messages,\n",
        "            temperature=0.0, # Set low for deterministic output if possible\n",
        "            max_tokens=2048, # Adjust as needed, ARC grids can be large\n",
        "            request_timeout=120 # Timeout in seconds\n",
        "        )\n",
        "        # Extract the content directly - assumes success and standard response structure\n",
        "        response_content = response.choices[0].message.content\n",
        "        print(\"Response received.\")\n",
        "        return response_content\n",
        "    except Exception as e:\n",
        "        print(f\"\\033[91mLLM API call failed: {e}\\033[0m\")\n",
        "        # In this simplified version, we just return None on failure\n",
        "        return None\n",
        "\n",
        "print(\"LLM interaction function 'get_llm_response' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ookay, Now lets add a Parser that can extract the Json from the llm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM response parsing function 'parse_llm_response_for_grid' defined.\n"
          ]
        }
      ],
      "source": [
        "# Add this new cell after the cell defining get_llm_response (Section 7)\n",
        "\n",
        "import re # Import the regular expression module\n",
        "\n",
        "def parse_llm_response_for_grid(response_content: Optional[str]) -> Optional[Grid]:\n",
        "    \"\"\"\n",
        "    Attempts to find and parse a JSON grid array from the LLM response,\n",
        "    prioritizing content within ```json ... ``` or ``` ... ``` code blocks.\n",
        "\n",
        "    Returns:\n",
        "        Optional[Grid]: The parsed grid if found and valid, otherwise None.\n",
        "    \"\"\"\n",
        "    if not response_content or not isinstance(response_content, str):\n",
        "        print(\"Parser received empty or non-string content.\")\n",
        "        return None\n",
        "\n",
        "    response_text = response_content.strip()\n",
        "    \n",
        "    # 1. Prioritize finding ```json [...] ``` code blocks\n",
        "    # Regex explanation:\n",
        "    # ```json       - Matches the opening ```json tag\n",
        "    # \\s*          - Matches any whitespace (including newlines)\n",
        "    # (\\[         - Start capturing group 1, matches the opening square bracket\n",
        "    #   .*?        - Matches any character (including newlines) non-greedily\n",
        "    #  \\])        - Matches the closing square bracket, end capturing group 1\n",
        "    # \\s*          - Matches any whitespace\n",
        "    # ```          - Matches the closing ``` tag\n",
        "    json_block_match = re.search(r\"```json\\s*(\\[.*?\\])\\s*```\", response_text, re.DOTALL)\n",
        "    \n",
        "    potential_json_str = None\n",
        "    if json_block_match:\n",
        "        potential_json_str = json_block_match.group(1).strip()\n",
        "        # print(\"Found JSON content inside ```json block.\")\n",
        "    else:\n",
        "        # 2. If no ```json block, look for a plain ``` [...] ``` block\n",
        "        code_block_match = re.search(r\"```\\s*(\\[.*?\\])\\s*```\", response_text, re.DOTALL)\n",
        "        if code_block_match:\n",
        "            potential_json_str = code_block_match.group(1).strip()\n",
        "            # print(\"Found JSON content inside plain ``` block.\")\n",
        "        else:\n",
        "            # 3. Fallback: Look for the first occurrence of [...] in the entire text\n",
        "            # This is less precise but might catch cases without code blocks.\n",
        "            first_bracket_match = re.search(r\"(\\[.*?\\])\", response_text, re.DOTALL)\n",
        "            if first_bracket_match:\n",
        "                 potential_json_str = first_bracket_match.group(1).strip()\n",
        "                 # print(\"Found JSON content directly in text (fallback).\")\n",
        "\n",
        "    if not potential_json_str:\n",
        "        # print(\"Could not find any potential JSON grid content.\")\n",
        "        return None\n",
        "\n",
        "    # 4. Attempt to parse the extracted string\n",
        "    try:\n",
        "        parsed_grid = json.loads(potential_json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        # print(f\"JSON decoding failed for extracted string: {e}\")\n",
        "        # print(f\"String was: '{potential_json_str}'\")\n",
        "        return None\n",
        "\n",
        "    # 5. Basic Validation (Is it a list of lists of ints? Is it rectangular?)\n",
        "    if not isinstance(parsed_grid, list):\n",
        "        # print(\"Parsed content is not a list.\")\n",
        "        return None\n",
        "    \n",
        "    row_len = -1\n",
        "    for i, row in enumerate(parsed_grid):\n",
        "        if not isinstance(row, list):\n",
        "            # print(f\"Row {i} is not a list.\")\n",
        "            return None\n",
        "        if i == 0:\n",
        "            row_len = len(row)\n",
        "        elif len(row) != row_len:\n",
        "             # print(f\"Grid rows have inconsistent lengths (row 0: {row_len}, row {i}: {len(row)}).\")\n",
        "             return None # Ensure rectangular grid\n",
        "\n",
        "        for j, cell in enumerate(row):\n",
        "            if not isinstance(cell, int):\n",
        "                # Try conversion for string digits, otherwise fail\n",
        "                if isinstance(cell, str) and cell.isdigit():\n",
        "                    try:\n",
        "                        parsed_grid[i][j] = int(cell) # Modify in place\n",
        "                    except ValueError:\n",
        "                        # print(f\"Cell ({i},{j}) is non-integer ('{cell}') and couldn't be converted.\")\n",
        "                        return None\n",
        "                else:\n",
        "                     # print(f\"Cell ({i},{j}) is not an integer ('{cell}').\")\n",
        "                     return None\n",
        "            # Optional: Check 0-9 range\n",
        "            # if not (0 <= parsed_grid[i][j] <= 9): return None \n",
        "            \n",
        "    # print(\"Successfully parsed and validated grid.\")\n",
        "    return parsed_grid\n",
        "\n",
        "print(\"LLM response parsing function 'parse_llm_response_for_grid' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Example: Run Strategy and Compare Output\n",
        "\n",
        "Let's select a task and a test case, prepare the prompt using our templates, get the LLM's raw response, and print it alongside the ground truth solution for manual comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Simple Example: Task '0934a4d8', Test Case 0 ---\n",
            "Sending request to model: gpt-4o...\n",
            "Response received.\n",
            "\n",
            "--- Parsing LLM Response ---\n",
            "\n",
            "--- Comparison --- \n",
            "\n",
            "Raw LLM Response (first 500 chars):\n",
            "```\n",
            "To solve this problem, we need to identify the pattern or transformation applied to the input grids to produce the output grids in the training examples. Let's analyze the given examples:\n",
            "\n",
            "1. **Example 1:**\n",
            "   - Input: A 30x30 grid.\n",
            "   - Output: A 9x4 grid.\n",
            "   - Observation: The output grid seems to be a subgrid extracted from the input grid. Specifically, it appears to be a 9x4 section from the bottom-right corner of the input grid.\n",
            "\n",
            "2. **Example 2:**\n",
            "   - Input: A 30x30 grid.\n",
            "   - Output: A 4x...\n",
            "```\n",
            "\n",
            "Parsed Predicted Grid (JSON):\n",
            "```json\n",
            "[[9,7,7,5],[7,5,9,7],[5,1,6,1],[5,7,5,9],[1,6,1,5],[7,5,9,7],[5,1,6,1],[5,7,5,9],[1,6,1,5]]\n",
            "```\n",
            "\n",
            "Ground Truth Output (JSON):\n",
            "```json\n",
            "[[7,7,9],[7,2,9],[7,2,9],[7,7,9],[4,4,7],[4,4,7],[6,6,1],[6,6,6],[1,6,1]]\n",
            "```\n",
            "\n",
            "\u001b[91mParsed Grid Match: Parsed prediction differs from ground truth.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration for Simple Example --- \n",
        "EXAMPLE_TASK_ID = '0934a4d8' # Choose a task ID that was loaded\n",
        "TEST_CASE_INDEX = 0          # Which test case within the task (usually 0)\n",
        "MODEL_NAME = 'gpt-4o'     # The LLM model to use\n",
        "# --- End Configuration ---\\n\",\n",
        "\n",
        "# Check if data is loaded\n",
        "if not all_task_data or EXAMPLE_TASK_ID not in all_task_data:\n",
        "    print(f\"\\033[91mError: Task data not loaded or Task ID '{EXAMPLE_TASK_ID}' not found.\\033[0m\")\n",
        "else:\n",
        "    print(f\"--- Running Simple Example: Task '{EXAMPLE_TASK_ID}', Test Case {TEST_CASE_INDEX} ---\")\n",
        "\n",
        "    # 1. Get Task Data Components\n",
        "    train_pairs = get_train_pairs(EXAMPLE_TASK_ID, all_task_data)\n",
        "    test_inputs = get_test_inputs(EXAMPLE_TASK_ID, all_task_data)\n",
        "    test_outputs_truth = get_test_outputs(EXAMPLE_TASK_ID, all_task_data)\n",
        "\n",
        "    # Check if the test case index is valid\n",
        "    if TEST_CASE_INDEX >= len(test_inputs):\n",
        "        print(f\"\\033[91mError: Test case index {TEST_CASE_INDEX} is out of bounds for task '{EXAMPLE_TASK_ID}' (has {len(test_inputs)} test cases).\\033[0m\")\n",
        "    else:\n",
        "        target_test_input = test_inputs[TEST_CASE_INDEX]\n",
        "        ground_truth_output = test_outputs_truth[TEST_CASE_INDEX]\n",
        "\n",
        "        # 2. Prepare Prompt Messages using Templates\n",
        "        messages = prepare_prompt_messages(\n",
        "            system_template=SYSTEM_PROMPT_TEMPLATE,\n",
        "            user_template=USER_PROMPT_TEMPLATE,\n",
        "            train_pairs=train_pairs,\n",
        "            test_input_grid=target_test_input\n",
        "        )\n",
        "        \n",
        "        # 3. Get LLM Response\n",
        "        raw_llm_response = get_llm_response(messages, MODEL_NAME)\n",
        "\n",
        "        # 4. Parse the LLM Response\n",
        "        print(\"\\n--- Parsing LLM Response ---\")\n",
        "        parsed_predicted_grid = parse_llm_response_for_grid(raw_llm_response)\n",
        "\n",
        "        # 5. Print Raw Response, Parsed Prediction, and Ground Truth\n",
        "        print(\"\\n--- Comparison --- \")\n",
        "        print(f\"\\nRaw LLM Response (first 500 chars):\\n```\\n{str(raw_llm_response)[:500]}...\\n```\" if raw_llm_response else \"Raw LLM Response: None\")\n",
        "        \n",
        "        if parsed_predicted_grid is not None:\n",
        "            parsed_grid_string = format_grid_to_string(parsed_predicted_grid)\n",
        "            print(f\"\\nParsed Predicted Grid (JSON):\\n```json\\n{parsed_grid_string}\\n```\")\n",
        "        else:\n",
        "            print(\"\\nParsed Predicted Grid: \\033[91mParsing Failed (None)\\033[0m\")\n",
        "\n",
        "        # Format ground truth for easy comparison\n",
        "        ground_truth_string = format_grid_to_string(ground_truth_output)\n",
        "        print(f\"\\nGround Truth Output (JSON):\\n```json\\n{ground_truth_string}\\n```\")\n",
        "        \n",
        "        # Compare parsed grid with ground truth using numpy for robustness\n",
        "        if parsed_predicted_grid is not None:\n",
        "            # Assume numpy is imported as np earlier\n",
        "            is_correct = np.array_equal(np.array(parsed_predicted_grid), np.array(ground_truth_output))\n",
        "\n",
        "            if is_correct:\n",
        "                 print(\"\\n\\033[92mParsed Grid Match: Parsed prediction matches ground truth!\\033[0m\")\n",
        "            else:\n",
        "                 print(\"\\n\\033[91mParsed Grid Match: Parsed prediction differs from ground truth.\\033[0m\")\n",
        "        else:\n",
        "            print(\"\\nCannot compare parsed grid as parsing failed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
