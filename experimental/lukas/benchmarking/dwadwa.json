{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interacting with the ARC Dataset and LLMs\n",
        "\n",
        "This notebook provides a foundational workflow for exploring the Abstraction and Reasoning Corpus (ARC) dataset using Large Language Models (LLMs).\n",
        "\n",
        "**Understanding ARC:**\n",
        "\n",
        "The ARC dataset, created by Fran√ßois Chollet, is designed to test abstract reasoning and intelligence, moving beyond simple pattern recognition. Each ARC task challenges a system to infer an underlying transformation rule from a few examples and then apply that rule to new, unseen inputs.\n",
        "\n",
        "**Task Structure:**\n",
        "\n",
        "Every ARC task consists of:\n",
        "*   **`train` pairs:** A small set (usually 2-5) of input/output grid examples. The goal is to *learn* the transformation rule by observing how the input grids change to become the output grids in these examples.\n",
        "*   **`test` pairs:** One or more input grids (and their corresponding *unseen* solution output grids). After inferring the rule from the `train` set, the system must apply it to the `test` input grids to generate the correct `test` output grids.\n",
        "\n",
        "**The Challenge:** The core challenge is the *abstraction* of the rule from the `train` examples and its *generalization* to the `test` inputs.\n",
        "\n",
        "**Evaluation Rule (ARC Prize):**\n",
        "\n",
        "For a task to be considered solved:\n",
        "1.  The system must generate the correct output grid for **every single `test` input grid** within that task.\n",
        "2.  Each prediction must be an *exact* match to the ground truth solution grid.\n",
        "3.  If a task has multiple `test` inputs, **all** of them must be solved correctly based on the single rule inferred from the `train` set.\n",
        "\n",
        "**This Notebook's Goal (Initial Steps):**\n",
        "\n",
        "This part focuses on the initial setup and data handling:\n",
        "1.  Setting up the environment (installing and importing libraries).\n",
        "2.  Performing a basic API call test (assuming credentials are set).\n",
        "3.  Loading ARC task data from JSON files.\n",
        "4.  Understanding the structure of the loaded data.\n",
        "5.  Providing helper functions to easily access specific parts of a task (train pairs, test inputs, test outputs).\n",
        "6.  Providing visualization tools to inspect tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Libraries and API Test\n",
        "\n",
        "First, we install and import the necessary libraries. We assume you have Python and pip installed.\n",
        "*   `python-dotenv`: To potentially load API keys from a `.env` file (though we won't explicitly check for the key here).\n",
        "*   `litellm`: To interact with LLM APIs.\n",
        "*   `numpy`: For numerical operations and grid comparisons.\n",
        "*   `matplotlib`: For visualizing the ARC grids.\n",
        "\n",
        "We'll also perform a minimal API call to ensure `litellm` is configured correctly and can reach the service. **Note:** This step assumes your API key (e.g., `OPENAI_API_KEY`) is already set as an environment variable or globally configured for `litellm`. If not set, this test call will fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking/Installing required libraries...\n",
            "Libraries found.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages if they aren't already installed\n",
        "print(\"Checking/Installing required libraries...\")\n",
        "try:\n",
        "    import litellm\n",
        "    import dotenv\n",
        "    import numpy\n",
        "    import matplotlib\n",
        "    print(\"Libraries found.\")\n",
        "except ImportError:\n",
        "    print(\"Installing python-dotenv, litellm, numpy, matplotlib...\")\n",
        "    %pip install -q python-dotenv litellm numpy matplotlib\n",
        "    print(\"Installation complete. You might need to restart the kernel.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union # Keep typing for clarity\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import litellm\n",
        "from litellm import completion\n",
        "\n",
        "# --- Visualization Imports --- \n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "%matplotlib inline \n",
        "# --- End Visualization Imports --- \n",
        "\n",
        "# Attempt to load environment variables from .env file (optional)\n",
        "load_dotenv()\n",
        "\n",
        "# Reduce LiteLLM's default logging verbosity for a cleaner output\n",
        "litellm.set_verbose = False\n",
        "\n",
        "print(\"Libraries imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Visualization Setup\n",
        "\n",
        "Define the standard color map and normalization used for plotting ARC grids. Also define helper functions for displaying grids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visualization constants and functions defined.\n"
          ]
        }
      ],
      "source": [
        "# --- ARC Visualization Constants --- \n",
        "ARC_COLORMAP = colors.ListedColormap(\n",
        "    ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
        "     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'] # Removed white as it's usually not in tasks\n",
        ")\n",
        "ARC_NORM = colors.Normalize(vmin=0, vmax=9) # Adjusted vmax based on colormap\n",
        "\n",
        "# --- Grid Visualization Function (Simplified) --- \n",
        "def show_grid(grid, title=None, figsize=None):\n",
        "    \"\"\"Displays a single ARC grid using matplotlib.\"\"\"\n",
        "    # Assume grid is List[List[int]]\n",
        "    if not grid or not grid[0]: return # Handle empty grid\n",
        "    if not figsize:\n",
        "        # Basic dynamic sizing, no complex checks\n",
        "        figsize = (len(grid[0]) * 0.5, len(grid) * 0.5) \n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.imshow(np.array(grid), cmap=ARC_COLORMAP, norm=ARC_NORM)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.grid(False)\n",
        "    if title:\n",
        "        ax.set_title(title, fontsize=10)\n",
        "    plt.show()\n",
        "\n",
        "# --- Task Visualization Function (Simplified) --- \n",
        "def visualize_task(task_data, task_solutions=None, title=\"ARC Task\"):\n",
        "    \"\"\"Plots a task with all train/test input-output pairs.\"\"\"\n",
        "    # Assumes task_data structure is {'train': [...], 'test': [...]}\n",
        "    train_examples = task_data.get('train', [])\n",
        "    test_examples = task_data.get('test', [])\n",
        "    has_solution = task_solutions is not None # Check if solutions were provided\n",
        "\n",
        "    num_train = len(train_examples)\n",
        "    num_test = len(test_examples)\n",
        "    total_cols = num_train + num_test\n",
        "    if total_cols == 0: return # Nothing to plot\n",
        "    \n",
        "    # Basic figure sizing\n",
        "    fig_width = total_cols * 2.5 \n",
        "    fig_height = 5\n",
        "    fig, axs = plt.subplots(2, total_cols, figsize=(fig_width, fig_height), squeeze=False)\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "\n",
        "    # Plot Train Examples\n",
        "    for idx, example in enumerate(train_examples):\n",
        "        # Added check for valid input/output format\n",
        "        if 'input' in example and isinstance(example['input'], list) and example['input']:\n",
        "            axs[0, idx].imshow(np.array(example['input']), cmap=ARC_COLORMAP, norm=ARC_NORM)\n",
        "        axs[0, idx].set_title(f\"Train {idx+1} In\")\n",
        "        axs[0, idx].axis('off')\n",
        "        if 'output' in example and isinstance(example['output'], list) and example['output']:\n",
        "             axs[1, idx].imshow(np.array(example['output']), cmap=ARC_COLORMAP, norm=ARC_NORM)\n",
        "        axs[1, idx].set_title(f\"Train {idx+1} Out\")\n",
        "        axs[1, idx].axis('off')\n",
        "\n",
        "    # Plot Test Examples\n",
        "    for idx, example in enumerate(test_examples):\n",
        "        col_idx = num_train + idx\n",
        "        if 'input' in example and isinstance(example['input'], list) and example['input']:\n",
        "            axs[0, col_idx].imshow(np.array(example['input']), cmap=ARC_COLORMAP, norm=ARC_NORM)\n",
        "        axs[0, col_idx].set_title(f\"Test {idx+1} In\")\n",
        "        axs[0, col_idx].axis('off')\n",
        "\n",
        "        output_grid_to_show = None\n",
        "        output_title = f\"Test {idx+1} Out: ?\"\n",
        "\n",
        "        if has_solution and idx < len(task_solutions) and isinstance(task_solutions[idx], list) and task_solutions[idx]:\n",
        "            output_grid_to_show = task_solutions[idx]\n",
        "            output_title = f\"Test {idx+1} Out\"\n",
        "        elif 'output' in example and isinstance(example['output'], list) and example['output']: # Fallback to output in test data\n",
        "            output_grid_to_show = example['output']\n",
        "            output_title = f\"Test {idx+1} Out\"\n",
        "        \n",
        "        if output_grid_to_show:\n",
        "             axs[1, col_idx].imshow(np.array(output_grid_to_show), cmap=ARC_COLORMAP, norm=ARC_NORM)\n",
        "             axs[1, col_idx].axis('off')\n",
        "        else:\n",
        "             axs[1, col_idx].axis('off') # Keep axis off for placeholder\n",
        "        axs[1, col_idx].set_title(output_title)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout slightly for suptitle\n",
        "    plt.show()\n",
        "\n",
        "print(\"Visualization constants and functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing minimal API test call...\n",
            "API test call successful (received a response).\n"
          ]
        }
      ],
      "source": [
        "# --- Minimal API Key Test --- \n",
        "print(\"Performing minimal API test call...\")\n",
        "try:\n",
        "    response = completion(\n",
        "        model=\"gpt-4o\", \n",
        "        messages=[{\"role\": \"user\", \"content\": \"Respond with just 'OK'.\"}],\n",
        "        max_tokens=2, \n",
        "        request_timeout=20 \n",
        "    )\n",
        "    print(\"API test call successful (received a response).\") \n",
        "except Exception as e:\n",
        "    print(f\"\\033[91mAPI test call failed: {e}\\033[0m\")\n",
        "    print(\"Please ensure your API key (e.g., OPENAI_API_KEY) is set correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "Loads ARC tasks from JSON files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Attempting to load data from: C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\ARC-AGI\\data\\evaluation\n",
            "Processing 120 task files from C:\\Users\\Lukhausen\\github\\Lepus\\experimental\\lukas\\ARC-AGI\\data\\evaluation...\n",
            "\n",
            "Successfully attempted to load 120 tasks.\n",
            "\n",
            "Example Task IDs loaded: ['0934a4d8', '135a2760', '136b0064', '13e47133', '142ca369', '16b78196', '16de56c4', '1818057f', '195c6913', '1ae2feb7']\n"
          ]
        }
      ],
      "source": [
        "# --- Type Definitions --- \n",
        "Grid = List[List[int]] \n",
        "TaskPair = Dict[str, Grid] \n",
        "TaskData = Dict[str, List[TaskPair]] \n",
        "\n",
        "def load_arc_tasks_simple(data_dir: str, limit: Optional[int] = None) -> Dict[str, TaskData]:\n",
        "    \"\"\"Loads ARC tasks from JSON files into a dictionary (simplified).\"\"\"\n",
        "    arc_path = Path(data_dir)\n",
        "    if not arc_path.is_dir():\n",
        "        print(f\"Error: ARC data directory not found: {arc_path.resolve()}\")\n",
        "        return {}\n",
        "\n",
        "    json_files = sorted(list(arc_path.glob(\"*.json\")))\n",
        "\n",
        "    if not json_files:\n",
        "        print(f\"Warning: No JSON files found in {arc_path.resolve()}\")\n",
        "        return {}\n",
        "\n",
        "    if limit:\n",
        "        json_files = json_files[:limit]\n",
        "\n",
        "    loaded_tasks: Dict[str, TaskData] = {}\n",
        "    print(f\"Processing {len(json_files)} task files from {arc_path.resolve()}...\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        task_id = json_file.stem\n",
        "        with open(json_file, 'r', encoding='utf-8') as f:\n",
        "            task_data = json.load(f)\n",
        "        loaded_tasks[task_id] = task_data \n",
        "\n",
        "    print(f\"\\nSuccessfully attempted to load {len(loaded_tasks)} tasks.\")\n",
        "    return loaded_tasks\n",
        "\n",
        "# --- Load the Data --- \n",
        "# <<< UPDATE THIS PATH >>>\n",
        "ARC_DATA_DIR = \"../ARC-AGI/data/evaluation\" # ADJUST THIS PATH!\n",
        "TASK_LOAD_LIMIT = None # Load only a few tasks for faster testing\n",
        "\n",
        "print(f\"\\nAttempting to load data from: {Path(ARC_DATA_DIR).resolve()}\")\n",
        "all_task_data: Dict[str, TaskData] = load_arc_tasks_simple(ARC_DATA_DIR, limit=TASK_LOAD_LIMIT)\n",
        "\n",
        "if not all_task_data:\n",
        "    print(\"\\n--- CRITICAL: No tasks were loaded. Check ARC_DATA_DIR path. --- \")\n",
        "else:\n",
        "    print(f\"\\nExample Task IDs loaded: {list(all_task_data.keys())[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding ARC Task Data Structure\n",
        "\n",
        "Data is loaded into `all_task_data`, a dictionary mapping `task_id` (string) to `TaskData`.\n",
        "`TaskData` is a dictionary, usually with keys `'train'` and `'test'`.\n",
        "`'train'` and `'test'` contain lists of `TaskPair` dictionaries.\n",
        "Each `TaskPair` has `'input'` and `'output'` keys, holding the grids (`Grid`, which is `List[List[int]]`).\n",
        "\n",
        "To solve a test case, the LLM needs all `train` pairs and the specific `test` input grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helper Functions for Accessing Task Components\n",
        "\n",
        "Simple accessors for train pairs and test inputs/outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined: get_train_pairs, get_test_inputs, get_test_outputs\n"
          ]
        }
      ],
      "source": [
        "def get_train_pairs(task_id: str, all_tasks: Dict[str, TaskData]) -> List[TaskPair]:\n",
        "    \"\"\"Returns the list of training pairs for a given task ID.\"\"\"\n",
        "    return all_tasks[task_id]['train']\n",
        "\n",
        "def get_test_inputs(task_id: str, all_tasks: Dict[str, TaskData]) -> List[Grid]:\n",
        "    \"\"\"Returns a list of all test input grids for a given task ID.\"\"\"\n",
        "    return [pair['input'] for pair in all_tasks[task_id]['test']]\n",
        "\n",
        "def get_test_outputs(task_id: str, all_tasks: Dict[str, TaskData]) -> List[Grid]:\n",
        "    \"\"\"Returns a list of all test output (solution) grids for a given task ID.\"\"\"\n",
        "    return [pair['output'] for pair in all_tasks[task_id]['test']]\n",
        "\n",
        "print(\"Helper functions defined: get_train_pairs, get_test_inputs, get_test_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Usage of Helper Functions\n",
        "\n",
        "Demonstrating the accessors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Example data extraction for Task ID: 0934a4d8 ---\n",
            "Number of training pairs: 4\n",
            "Structure of first train pair: dict_keys(['input', 'output'])\n",
            "\n",
            "Number of test inputs: 1\n",
            "Type of first test input: <class 'list'>\n",
            "\n",
            "Number of test outputs (solutions): 1\n",
            "Type of first test solution: <class 'list'>\n"
          ]
        }
      ],
      "source": [
        "if all_task_data:\n",
        "    example_task_id = list(all_task_data.keys())[0] \n",
        "    print(f\"--- Example data extraction for Task ID: {example_task_id} ---\")\n",
        "\n",
        "    train_pairs = get_train_pairs(example_task_id, all_task_data)\n",
        "    print(f\"Number of training pairs: {len(train_pairs)}\")\n",
        "    if train_pairs: print(f\"Structure of first train pair: {train_pairs[0].keys()}\") \n",
        "\n",
        "    test_inputs = get_test_inputs(example_task_id, all_task_data)\n",
        "    print(f\"\\nNumber of test inputs: {len(test_inputs)}\")\n",
        "    if test_inputs: print(f\"Type of first test input: {type(test_inputs[0])}\")\n",
        "\n",
        "    test_outputs = get_test_outputs(example_task_id, all_task_data)\n",
        "    print(f\"\\nNumber of test outputs (solutions): {len(test_outputs)}\")\n",
        "    if test_outputs: print(f\"Type of first test solution: {type(test_outputs[0])}\")\n",
        "else:\n",
        "    print(\"\\nNo task data loaded, skipping example.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Visualizing a Task\n",
        "\n",
        "Using the visualization function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Visualizing Task ID: 0934a4d8 ---\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJwAAAHgCAYAAADg9nfBAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaj9JREFUeJzt3Xl4ldW9/v97W8YwRkBEPEwpWBwKASURRFqR4EAAAAAAAAjh18wwkAAAAAAABBseAEAAAAAACAoFhwAgAAAAAAQFAsOAEAAAAAACAoFpwAAAAAAAAQFAtOAAAAAAAACIoFJwAAAAAAAATFghMAAAAAAACCYsEJAAAAAAAAQf1/LhbtFtPGzl4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1250x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if all_task_data:\n",
        "    example_task_id = list(all_task_data.keys())[0]\n",
        "    print(f\"--- Visualizing Task ID: {example_task_id} ---\")\n",
        "    \n",
        "    task_data_to_visualize = all_task_data[example_task_id]\n",
        "    ground_truth_solutions = get_test_outputs(example_task_id, all_task_data)\n",
        "    \n",
        "    visualize_task(task_data_to_visualize, \n",
        "                     task_solutions=ground_truth_solutions, \n",
        "                     title=f\"Task: {example_task_id}\")\n",
        "else:\n",
        "    print(\"\\nNo task data loaded, skipping visualization example.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Prompt Strategy Templates\n",
        "\n",
        "String templates for system and user messages, using placeholders like `{train_examples_string}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt templates defined (SYSTEM_PROMPT_TEMPLATE, USER_PROMPT_TEMPLATE).\n"
          ]
        }
      ],
      "source": [
        "# Define simple string templates for the prompts\n",
        "SYSTEM_PROMPT_TEMPLATE = (\n",
        "\"\"\"\n",
        "You will be provided with example inputs and outputs. Analyze the train examples. These tasks follow the style of ARC (Abstraction and Reasoning Corpus) problems, where the objective is to deduce transformation rules from visual or structural patterns.\n",
        "\n",
        "Your goal is to find common rules that are applied to the input to be transformed into the output.  \n",
        "To achieve this, do the following:  \n",
        "1. Find possible rules that could be applied in combination to achieve the transformation from the input to the output. Be really precise in the rule definition. What transformations have to be applied exactly? What are they based upon?  \n",
        "2. Test those rules by applying them to all the available train examples and seeing if they reproduce the desired output. You have to verify that the deduced ruleset actually works with the train examples before proceeding to the test.  \n",
        "If the desired output is achieved in all present examples, then apply those found rules to the given test input.  \n",
        "If the ruleset you deduced fails at any of the train examples, begin again from step one and modify the rules you deduce.  \n",
        "Then test again for all train examples before proceeding to the test. (Output your final solution as a JSON array in a code block)\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Adjusted template slightly to ensure good spacing with multi-line grids\n",
        "USER_PROMPT_TEMPLATE = (\n",
        "    \"**TRAIN EXAMPLES:**\\n\\n\"\n",
        "    \"{train_examples_string}\\n\\n\\n\\n\"\n",
        "    \"**TEST INPUT GRID:**\\n\\n\"\n",
        "    \"{test_input_string}\\n\\n\"\n",
        ")\n",
        "\n",
        "print(\"Prompt templates defined (SYSTEM_PROMPT_TEMPLATE, USER_PROMPT_TEMPLATE).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Formatting Helpers and Prompt Preparation\n",
        "\n",
        "Functions to format grids into the desired string representation (space-separated numbers, multi-line) and prepare the final LLM messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatting and prompt preparation functions defined (with grid string format).\n"
          ]
        }
      ],
      "source": [
        "# --- Formatting Helpers (Modified for Grid String) --- \n",
        "\n",
        "def format_grid_to_string(grid: Grid) -> str:\n",
        "    \"\"\"Converts a grid (list of lists) to a multi-line string representation.\"\"\"\n",
        "    # Assume grid is List[List[int]]\n",
        "    if not grid or not grid[0]: return \"\" # Handle empty grid\n",
        "    return \"\\n\".join(\" \".join(map(str, row)) for row in grid)\n",
        "\n",
        "def format_train_pairs_to_string(train_pairs: List[TaskPair]) -> str:\n",
        "    \"\"\"Formats a list of training pairs into a single string for the prompt (multi-line grid format).\"\"\"\n",
        "    formatted_pairs = []\n",
        "    for i, pair in enumerate(train_pairs):\n",
        "        # Assume 'input' and 'output' keys exist\n",
        "        input_str = format_grid_to_string(pair['input'])\n",
        "        output_str = format_grid_to_string(pair['output'])\n",
        "        # Add extra newlines for clarity between examples and between input/output\n",
        "        formatted_pairs.append(f\"Example {i+1} Input:\\n{input_str}\\n\\nExample {i+1} Solution:\\n{output_str}\")\n",
        "    # Join examples with double newlines\n",
        "    return \"\\n\\n\".join(formatted_pairs)\n",
        "\n",
        "# --- Prompt Preparation Function (No changes needed) --- \n",
        "\n",
        "def prepare_prompt_messages(\n",
        "    system_template: str,\n",
        "    user_template: str,\n",
        "    train_pairs: List[TaskPair],\n",
        "    test_input_grid: Grid\n",
        ") -> List[Dict[str, str]]:\n",
        "    \"\"\"Formats data using the new formatters and inserts into prompt templates.\"\"\"\n",
        "    train_examples_str = format_train_pairs_to_string(train_pairs)\n",
        "    test_input_str = format_grid_to_string(test_input_grid)\n",
        "\n",
        "    final_user_prompt = user_template.format(\n",
        "        train_examples_string=train_examples_str,\n",
        "        test_input_string=test_input_str\n",
        "    )\n",
        "    \n",
        "    final_system_prompt = system_template\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": final_system_prompt},\n",
        "        {\"role\": \"user\", \"content\": final_user_prompt}\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "print(\"Formatting and prompt preparation functions defined (with grid string format).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Preparing messages using sample data (Grid String Format) ---\n",
            "\n",
            "Resulting 'messages' list (ready for LLM API):\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"\\nYou will be provided with example inputs and outputs. Analyze the train examples. These tasks follow the style of ARC (Abstraction and Reasoning Corpus) problems, where the objective is to deduce transformation rules from visual or structural patterns.\\n\\nYour goal is to find common rules that are applied to the input to be transformed into the output.  \\nTo achieve this, do the following:  \\n1. Find possible rules that could be applied in combination to achieve the transformation from the input to the output. Be really precise in the rule definition. What transformations have to be applied exactly? What are they based upon?  \\n2. Test those rules by applying them to all the available train examples and seeing if they reproduce the desired output. You have to verify that the deduced ruleset actually works with the train examples before proceeding to the test.  \\nIf the desired output is achieved in all present examples, then apply those found rules to the given test input.  \\nIf the ruleset you deduced fails at any of the train examples, begin again from step one and modify the rules you deduce.  \\nThen test again for all train examples before proceeding to the test. (Output your final solution as a JSON array in a code block)\\n\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"**TRAIN EXAMPLES:**\\n\\nExample 1 Input:\\n1 0\\n0 0\\n\\nExample 1 Solution:\\n0 1\\n0 0\\n\\nExample 2 Input:\\n0 0\\n2 0\\n\\nExample 2 Solution:\\n0 0\\n0 2\\n\\n\\n\\n**TEST INPUT GRID:**\\n\\n0 3\\n0 0\\n\\n\"\n",
            "  }\n",
            "]\n",
            "\n",
            "--- Content of the 'user' message: ---\n",
            "**TRAIN EXAMPLES:**\n",
            "\n",
            "Example 1 Input:\n",
            "1 0\n",
            "0 0\n",
            "\n",
            "Example 1 Solution:\n",
            "0 1\n",
            "0 0\n",
            "\n",
            "Example 2 Input:\n",
            "0 0\n",
            "2 0\n",
            "\n",
            "Example 2 Solution:\n",
            "0 0\n",
            "0 2\n",
            "\n",
            "\n",
            "\n",
            "**TEST INPUT GRID:**\n",
            "\n",
            "0 3\n",
            "0 0\n",
            "\n",
            "\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- Sample Data for Testing Prompt Preparation --- \n",
        "sample_train_pairs: List[TaskPair] = [\n",
        "    {'input': [[1, 0], [0, 0]], 'output': [[0, 1], [0, 0]]},\n",
        "    {'input': [[0, 0], [2, 0]], 'output': [[0, 0], [0, 2]]}\n",
        "]\n",
        "sample_test_input_grid: Grid = [[0, 3], [0, 0]]\n",
        "\n",
        "# --- Call the Preparation Function --- \n",
        "print(\"--- Preparing messages using sample data (Grid String Format) ---\")\n",
        "prepared_messages = prepare_prompt_messages(\n",
        "    system_template=SYSTEM_PROMPT_TEMPLATE,\n",
        "    user_template=USER_PROMPT_TEMPLATE,\n",
        "    train_pairs=sample_train_pairs,\n",
        "    test_input_grid=sample_test_input_grid\n",
        ")\n",
        "\n",
        "# --- Print the Result --- \n",
        "print(\"\\nResulting 'messages' list (ready for LLM API):\")\n",
        "print(json.dumps(prepared_messages, indent=2))\n",
        "\n",
        "# --- Print the formatted User prompt content for clarity ---\n",
        "print(\"\\n--- Content of the 'user' message: ---\")\n",
        "print(prepared_messages[1]['content'])\n",
        "print(\"--------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Get LLM Response (Simple)\n",
        "\n",
        "Sends messages to the LLM and returns the raw response content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM interaction function 'get_llm_response' defined.\n"
          ]
        }
      ],
      "source": [
        "def get_llm_response(messages: List[Dict[str, str]], model_name: str) -> Optional[str]:\n",
        "    \"\"\"Sends messages to the LLM and returns the raw response content.\"\"\"\n",
        "    # print(f\"Sending request to model: {model_name}...\") # Reduced verbosity\n",
        "    try:\n",
        "        response = completion(\n",
        "            model=model_name,\n",
        "            messages=messages,\n",
        "            request_timeout=180,\n",
        "            num_retries = 3\n",
        "        )\n",
        "        response_content = response.choices[0].message.content\n",
        "        # print(\"Response received.\") # Reduced verbosity\n",
        "        return response_content\n",
        "    except Exception as e:\n",
        "        print(f\"\\033[91mLLM API call failed: {e}\\033[0m\")\n",
        "        return None \n",
        "\n",
        "print(\"LLM interaction function 'get_llm_response' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 LLM Response Parser\n",
        "\n",
        "Extracts the JSON grid from the LLM response (still expects JSON output from LLM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM response parsing function 'parse_llm_response_for_grid' defined.\n"
          ]
        }
      ],
      "source": [
        "import re # Import the regular expression module\n",
        "\n",
        "def parse_llm_response_for_grid(response_content: Optional[str]) -> Optional[Grid]:\n",
        "    \"\"\"Attempts to find and parse a JSON grid array from the LLM response.\"\"\"\n",
        "    if not response_content or not isinstance(response_content, str):\n",
        "        return None\n",
        "\n",
        "    response_text = response_content.strip()\n",
        "    \n",
        "    # Prioritize ```json [...] ``` blocks\n",
        "    json_block_match = re.search(r\"```json\\s*(\\[.*?\\])\\s*```\", response_text, re.DOTALL)\n",
        "    potential_json_str = None\n",
        "    if json_block_match:\n",
        "        potential_json_str = json_block_match.group(1).strip()\n",
        "    else:\n",
        "        # Fallback to plain ``` [...] ``` blocks\n",
        "        code_block_match = re.search(r\"```\\s*(\\[.*?\\])\\s*```\", response_text, re.DOTALL)\n",
        "        if code_block_match:\n",
        "            potential_json_str = code_block_match.group(1).strip()\n",
        "        else:\n",
        "            # Fallback: Find the first plausible JSON array `[[...]]` anywhere\n",
        "            first_bracket_match = re.search(r\"(\\[\\s*\\[.*?\\]\\s*\\])\", response_text, re.DOTALL)\n",
        "            if first_bracket_match:\n",
        "                 potential_json_str = first_bracket_match.group(1).strip()\n",
        "\n",
        "    if not potential_json_str:\n",
        "        return None\n",
        "\n",
        "    # Attempt to parse the extracted string\n",
        "    try:\n",
        "        parsed_grid = json.loads(potential_json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        return None\n",
        "\n",
        "    # Basic Validation: Is it a list of lists?\n",
        "    if not isinstance(parsed_grid, list):\n",
        "        return None\n",
        "    if not all(isinstance(row, list) for row in parsed_grid):\n",
        "         return None\n",
        "\n",
        "    return parsed_grid\n",
        "\n",
        "print(\"LLM response parsing function 'parse_llm_response_for_grid' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Example: Run Strategy and Compare Output\n",
        "\n",
        "Run the process for a single test case: prepare prompt (with grid strings), call LLM, parse response (expecting JSON), compare, visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Messages Sent to LLM --- \n",
            "System Message:\n",
            "\n",
            "You will be provided with example inputs and outputs. Analyze the train examples. These tasks follow the style of ARC (Abstraction and Reasoning Corpus) problems, where the objective is to deduce transformation rules from visual or structural patterns.\n",
            "\n",
            "Your goal is to find common rules that are applied to the input to be transformed into the output.  \n",
            "To achieve this, do the following:  \n",
            "1. Find possible rules that could be applied in combination to achieve the transformation from the input to the output. Be really precise in the rule definition. What transformations have to be applied exactly? What are they based upon?  \n",
            "2. Test those rules by applying them to all the available train examples and seeing if they reproduce the desired output. You have to verify that the deduced ruleset actually works with the train examples before proceeding to the test.  \n",
            "If the desired output is achieved in all present examples, then apply those found rules to the given test input.  \n",
            "If the ruleset you deduced fails at any of the train examples, begin again from step one and modify the rules you deduce.  \n",
            "Then test again for all train examples before proceeding to the test. (Output your final solution as a JSON array in a code block)\n",
            "\n",
            "\n",
            "User Message:\n",
            "**TRAIN EXAMPLES:**\n",
            "\n",
            "Example 1 Input:\n",
            "3 5 3 3 6 6 5 4 1 4 9 9 4 3 9 9 9 9 3 4 9 9 4 1 4 5 6 6 3 3\n",
            "5 3 3 3 6 6 4 5 4 1 9 9 3 4 9 1 1 9 4 3 9 9 1 4 5 4 6 6 3 3\n",
            "1 1 3 5 5 4 6 6 9 1 1 4 9 9 4 5 5 4 9 9 4 1 1 9 6 6 4 5 5 3\n",
            "1 1 5 3 4 5 6 6 1 9 4 1 9 1 4 4 4 4 1 9 1 4 9 1 6 6 5 4 3 5\n",
            "6 9 9 9 3 5 3 3 4 3 9 9 9 2 6 9 9 6 2 9 9 9 3 4 3 3 5 3 9 9\n",
            "9 6 9 9 5 3 3 3 3 4 9 1 9 9 9 6 6 9 9 9 1 9 4 3 3 3 3 5 9 9\n",
            "9 9 6 9 1 1 3 5 9 9 4 4 6 9 9 2 2 9 9 6 4 4 9 9 5 3 1 1 9 6\n",
            "9 9 9 6 1 1 5 3 9 1 5 4 9 6 9 9 9 9 6 9 4 5 1 9 3 5 1 1 6 9\n",
            "1 4 9 1 4 3 9 9 5 5 7 2 4 3 2 4 4 2 3 4 2 7 5 5 9 9 3 4 1 9\n",
            "4 1 1 9 3 4 9 1 4 5 2 7 3 4 4 2 2 4 4 3 7 2 5 4 1 9 4 3 9 1\n",
            "9 9 1 4 9 9 4 5 6 4 5 5 2 4 4 3 3 4 4 2 5 5 4 6 5 4 9 9 4 1\n",
            "9 9 4 1 9 1 4 4 4 5 4 5 4 2 3 4 4 3 2 4 5 4 5 4 4 4 1 9 1 4\n",
            "4 3 9 9 9 9 6 9 5 9 7 7 5 5 7 2 2 7 5 5 7 7 9 5 9 6 9 9 9 9\n",
            "3 4 9 1 2 9 9 6 9 5 7 7 4 5 2 7 7 2 5 4 7 7 5 9 6 9 9 2 1 9\n",
            "9 9 4 4 6 9 9 9 7 7 5 9 5 4 5 5 5 5 4 5 9 5 7 7 9 8 8 8 8 4\n",
            "9 1 5 4 9 6 2 9 7 7 9 5 4 6 4 5 5 4 6 4 5 9 7 7 9 8 8 8 8 5\n",
            "9 1 5 4 9 6 2 9 7 7 9 5 4 6 4 5 5 4 6 4 5 9 7 7 9 8 8 8 8 5\n",
            "9 9 4 4 6 9 9 9 7 7 5 9 5 4 5 5 5 5 4 5 9 5 7 7 9 8 8 8 8 4\n",
            "3 4 9 1 2 9 9 6 9 5 7 7 4 5 2 7 7 2 5 4 7 7 5 9 6 8 8 8 8 9\n",
            "4 3 9 9 9 9 6 9 5 9 7 7 5 5 7 2 2 7 5 5 7 7 9 5 9 8 8 8 8 9\n",
            "9 9 4 1 9 1 4 4 4 5 4 5 4 2 3 4 4 3 2 4 5 4 5 4 4 8 8 8 8 4\n",
            "9 9 1 4 9 9 4 5 6 4 5 5 2 4 4 3 3 4 4 2 5 5 4 6 5 8 8 8 8 1\n",
            "4 1 1 9 3 4 9 1 4 5 2 7 3 4 4 2 2 4 4 3 7 2 5 4 1 8 8 8 8 1\n",
            "1 4 9 1 4 3 9 9 5 5 7 2 4 3 2 4 4 2 3 4 2 7 5 5 9 9 3 4 1 9\n",
            "9 9 9 6 1 1 5 3 9 1 5 4 9 6 9 9 9 9 6 9 4 5 1 9 3 5 1 1 6 9\n",
            "9 9 6 9 1 1 3 5 9 9 4 4 6 9 9 2 2 9 9 6 4 4 9 9 5 3 1 1 9 6\n",
            "9 6 9 9 5 3 3 3 3 4 9 1 9 9 9 6 6 9 9 9 1 9 4 3 3 3 3 5 9 9\n",
            "6 9 9 9 3 5 3 3 4 3 9 9 9 2 6 9 9 6 2 9 9 9 3 4 3 3 5 3 9 9\n",
            "1 1 5 3 4 5 6 6 1 9 4 1 9 1 4 4 4 4 1 9 1 4 9 1 6 6 5 4 3 5\n",
            "1 1 3 5 5 4 6 6 9 1 1 4 9 9 4 5 5 4 9 9 4 1 1 9 6 6 4 5 5 3\n",
            "\n",
            "Example 1 Solution:\n",
            "9 9 6 4\n",
            "2 6 9 4\n",
            "2 6 9 4\n",
            "9 9 6 4\n",
            "9 9 2 1\n",
            "6 9 9 9\n",
            "4 1 9 1\n",
            "4 9 9 4\n",
            "9 4 3 9\n",
            "\n",
            "Example 2 Input:\n",
            "9 9 2 3 4 4 7 5 3 3 6 6 3 5 6 4 4 6 5 3 6 6 3 3 5 7 4 4 3 2\n",
            "7 9 3 5 4 4 5 7 3 3 6 6 6 3 4 6 6 4 3 6 6 6 3 3 7 5 4 4 5 3\n",
            "3 2 9 9 7 5 4 4 4 1 3 3 6 4 4 7 7 4 4 6 3 8 8 8 8 8 5 7 9 9\n",
            "2 3 7 9 5 7 4 4 1 4 3 3 4 6 7 4 4 7 6 4 3 8 8 8 8 8 7 5 9 7\n",
            "7 7 9 3 9 9 5 3 3 6 6 4 6 7 9 9 9 9 7 6 4 8 8 8 8 8 9 9 3 9\n",
            "7 7 3 9 7 9 3 2 5 3 4 6 2 6 9 9 9 9 6 2 6 8 8 8 8 8 9 7 9 3\n",
            "9 3 7 7 3 2 9 9 6 4 4 7 9 2 6 7 7 6 2 9 7 4 4 6 9 9 2 3 7 7\n",
            "3 9 7 7 2 3 7 9 4 6 7 4 2 9 2 6 6 2 9 2 4 7 6 4 9 7 3 2 7 7\n",
            "3 3 4 1 3 5 6 4 2 4 7 7 1 6 7 2 2 7 6 1 7 7 4 2 4 6 5 3 1 4\n",
            "3 3 1 4 6 3 4 6 2 2 7 1 6 1 2 7 7 2 1 6 1 7 2 2 6 4 3 6 4 1\n",
            "6 6 3 3 6 4 4 7 1 1 2 4 7 2 1 6 6 1 2 7 4 2 1 1 7 4 4 6 3 3\n",
            "6 6 3 3 4 6 7 4 1 3 2 2 2 7 6 1 1 6 7 2 2 2 3 1 4 7 6 4 3 3\n",
            "3 6 6 4 6 2 9 2 9 9 9 7 2 4 1 7 7 1 4 2 7 9 9 9 2 9 2 6 4 6\n",
            "5 3 4 6 7 6 2 9 9 9 7 9 2 2 7 7 7 7 2 2 9 7 9 9 9 2 6 7 6 4\n",
            "6 4 4 7 9 9 6 2 9 7 9 9 3 1 2 4 4 2 1 3 9 9 7 9 2 6 9 9 7 4\n",
            "4 6 7 4 9 9 7 6 7 9 9 9 1 1 2 2 2 2 1 1 9 9 9 7 6 7 9 9 4 7\n",
            "4 6 7 4 9 9 7 6 7 9 9 9 1 1 2 2 2 2 1 1 9 9 9 7 6 7 9 9 4 7\n",
            "6 4 4 7 9 9 6 2 9 7 9 9 3 1 2 4 4 2 1 3 9 9 7 9 2 6 9 9 7 4\n",
            "5 3 4 6 7 6 2 9 9 9 7 9 2 2 7 7 7 7 2 2 9 7 9 9 9 2 6 7 6 4\n",
            "3 6 6 4 6 2 9 2 9 9 9 7 2 4 1 7 7 1 4 2 7 9 9 9 2 9 2 6 4 6\n",
            "6 6 3 3 4 6 7 4 1 3 2 2 2 7 6 1 1 6 7 2 2 2 3 1 4 7 6 4 3 3\n",
            "6 6 3 3 6 4 4 7 1 1 2 4 7 2 1 6 6 1 2 7 4 2 1 1 7 4 4 6 3 3\n",
            "3 3 1 4 6 3 4 6 2 2 7 1 6 1 2 7 7 2 1 6 1 7 2 2 6 4 3 6 4 1\n",
            "3 3 4 1 3 5 6 4 2 4 7 7 1 6 7 2 2 7 6 1 7 7 4 2 4 6 5 3 1 4\n",
            "3 9 7 7 2 3 7 9 4 6 7 4 2 9 2 6 6 2 9 2 4 7 6 4 9 7 3 2 7 7\n",
            "9 3 7 7 3 2 9 9 6 4 4 7 9 2 6 7 7 6 2 9 7 4 4 6 9 9 2 3 7 7\n",
            "7 7 3 9 7 9 3 2 5 3 4 6 2 6 9 9 9 9 6 2 6 4 3 5 2 3 9 7 9 3\n",
            "7 7 9 3 9 9 5 3 3 6 6 4 6 7 9 9 9 9 7 6 4 6 6 3 3 5 9 9 3 9\n",
            "2 3 7 9 5 7 4 4 1 4 3 3 4 6 7 4 4 7 6 4 3 3 4 1 4 4 7 5 9 7\n",
            "3 2 9 9 7 5 4 4 4 1 3 3 6 4 4 7 7 4 4 6 3 3 1 4 4 4 5 7 9 9\n",
            "\n",
            "Example 2 Solution:\n",
            "3 1 4 4 4\n",
            "3 4 1 4 4\n",
            "6 6 3 3 5\n",
            "4 3 5 2 3\n",
            "\n",
            "Example 3 Input:\n",
            "1 9 4 4 9 9 2 7 6 6 9 9 7 6 7 2 2 7 6 7 9 9 6 6 7 2 9 9 4 4\n",
            "7 1 4 4 9 9 7 2 6 6 9 9 6 7 2 7 7 2 7 6 9 9 6 6 2 7 9 9 4 4\n",
            "2 7 1 9 2 7 9 9 4 4 6 6 7 2 5 1 1 5 2 7 6 6 4 4 9 9 7 2 9 1\n",
            "7 2 7 1 7 2 9 9 4 4 6 6 2 7 5 5 5 5 7 2 6 6 4 4 9 9 2 7 1 7\n",
            "9 6 7 2 1 9 4 4 7 6 7 2 9 2 6 4 4 6 2 9 2 7 6 7 4 4 9 1 2 7\n",
            "6 9 2 7 7 1 4 4 6 7 2 7 9 9 4 6 6 4 9 9 7 2 7 6 4 4 1 7 7 2\n",
            "7 2 9 6 2 7 1 9 7 2 5 5 4 5 9 2 2 9 5 4 5 5 2 7 9 1 7 2 6 9\n",
            "2 7 6 9 7 2 7 1 2 7 1 5 5 4 9 9 9 9 4 5 5 1 7 2 1 7 2 7 9 6\n",
            "6 6 4 4 7 6 7 2 3 7 1 4 9 7 7 6 6 7 7 9 4 1 7 3 2 7 6 7 4 4\n",
            "6 6 4 4 6 7 2 7 4 3 4 4 7 9 6 7 7 6 9 7 4 4 3 4 7 2 7 6 4 4\n",
            "9 9 6 6 7 2 5 1 3 7 3 7 7 6 9 7 7 9 6 7 7 3 7 3 1 5 2 7 6 6\n",
            "9 9 6 6 2 7 5 5 7 7 4 3 6 7 7 9 9 7 7 6 3 4 7 7 5 5 7 2 6 6\n",
            "7 6 7 2 9 9 4 5 6 6 5 9 3 7 4 4 4 4 7 3 9 5 6 6 5 4 9 9 2 7\n",
            "6 7 2 7 2 9 5 4 6 6 9 5 4 3 4 1 1 4 3 4 5 9 6 6 4 5 9 2 7 2\n",
            "7 2 5 5 6 4 9 9 5 9 6 6 7 7 3 7 7 3 7 7 6 6 9 5 9 9 4 6 5 5\n",
            "2 7 1 5 4 6 2 9 9 5 6 6 7 3 4 3 3 4 3 7 6 6 5 9 9 2 6 4 5 1\n",
            "2 7 1 5 4 6 2 9 9 5 6 6 7 3 4 3 3 4 3 7 6 6 5 9 9 2 6 4 5 1\n",
            "7 2 5 5 6 4 9 9 5 9 6 6 7 7 3 7 7 3 7 7 6 6 9 5 9 9 4 6 5 5\n",
            "6 7 2 7 2 9 5 4 6 6 9 5 4 3 4 1 1 4 3 4 5 9 6 6 4 5 9 2 7 2\n",
            "7 6 7 2 9 9 4 5 6 6 5 9 8 8 8 8 8 8 8 3 9 5 6 6 5 4 9 9 2 7\n",
            "9 9 6 6 2 7 5 5 7 7 4 3 8 8 8 8 8 8 8 6 3 4 7 7 5 5 7 2 6 6\n",
            "9 9 6 6 7 2 5 1 3 7 3 7 8 8 8 8 8 8 8 7 7 3 7 3 1 5 2 7 6 6\n",
            "6 6 4 4 6 7 2 7 4 3 4 4 7 9 6 7 7 6 9 7 4 4 3 4 7 2 7 6 4 4\n",
            "6 6 4 4 7 6 7 2 3 7 1 4 9 7 7 6 6 7 7 9 4 1 7 3 2 7 6 7 4 4\n",
            "2 7 6 9 7 2 7 1 2 7 1 5 5 4 9 9 9 9 4 5 5 1 7 2 1 7 2 7 9 6\n",
            "7 2 9 6 2 7 1 9 7 2 5 5 4 5 9 2 2 9 5 4 5 5 2 7 9 1 7 2 6 9\n",
            "6 9 2 7 7 1 4 4 6 7 2 7 9 9 4 6 6 4 9 9 7 2 7 6 4 4 1 7 7 2\n",
            "9 6 7 2 1 9 4 4 7 6 7 2 9 2 6 4 4 6 2 9 2 7 6 7 4 4 9 1 2 7\n",
            "7 2 7 1 7 2 9 9 4 4 6 6 2 7 5 5 5 5 7 2 6 6 4 4 9 9 2 7 1 7\n",
            "2 7 1 9 2 7 9 9 4 4 6 6 7 2 5 1 1 5 2 7 6 6 4 4 9 9 7 2 9 1\n",
            "\n",
            "Example 3 Solution:\n",
            "3 7 4 4 4 4 7\n",
            "6 7 7 9 9 7 7\n",
            "7 6 9 7 7 9 6\n",
            "\n",
            "Example 4 Input:\n",
            "3 1 1 9 5 6 7 1 1 4 5 7 3 9 9 1 1 9 9 3 7 5 4 1 1 7 6 5 9 1\n",
            "1 3 9 5 6 5 1 7 4 1 7 5 4 3 1 3 3 1 3 4 5 7 1 4 7 1 5 6 5 9\n",
            "6 9 3 1 7 1 5 6 9 9 1 4 9 1 1 4 4 1 1 9 4 1 9 9 6 5 1 7 1 3\n",
            "9 1 1 3 1 7 6 5 9 9 4 1 1 3 4 1 1 4 3 1 1 4 9 9 5 6 7 1 3 1\n",
            "6 6 6 7 3 1 5 9 3 4 9 1 6 7 2 5 5 2 7 6 1 9 4 3 9 5 1 3 7 6\n",
            "6 6 7 6 1 3 9 1 9 3 1 3 7 6 5 2 2 5 6 7 3 1 3 9 1 9 3 1 6 7\n",
            "6 7 6 6 1 9 3 1 9 1 1 4 6 9 6 7 7 6 9 6 4 1 1 9 1 3 9 1 6 6\n",
            "7 6 6 6 9 6 1 3 1 3 4 1 9 6 7 6 6 7 6 9 1 4 3 1 3 1 8 8 8 8\n",
            "1 4 9 9 3 9 9 1 1 1 6 1 5 2 5 5 5 5 2 5 1 6 1 1 1 9 8 8 8 8\n",
            "4 1 9 9 4 3 1 3 1 1 1 6 2 5 5 5 5 5 5 2 6 1 1 1 3 1 8 8 8 8\n",
            "5 7 1 4 9 1 1 4 2 2 1 1 5 5 5 2 2 5 5 5 1 1 2 2 4 1 8 8 8 8\n",
            "7 5 4 1 1 3 4 1 2 1 1 1 5 5 2 5 5 2 5 5 1 1 1 2 1 4 3 1 1 4\n",
            "3 4 9 1 6 7 6 9 7 6 3 3 1 1 6 1 1 6 1 1 3 3 6 7 9 6 7 6 1 9\n",
            "9 3 1 3 7 6 9 6 6 7 3 3 1 1 1 6 6 1 1 1 3 3 7 6 6 9 6 7 3 1\n",
            "9 1 1 4 2 5 6 7 3 3 7 6 1 2 1 1 1 1 2 1 6 7 3 3 7 6 5 2 4 1\n",
            "1 3 4 1 5 2 7 6 3 3 6 7 2 2 1 1 1 1 2 2 7 6 3 3 6 7 2 5 1 4\n",
            "1 3 4 1 5 2 7 6 3 3 6 7 2 2 1 1 1 1 2 2 7 6 3 3 6 7 2 5 1 4\n",
            "9 1 1 4 2 5 6 7 3 3 7 6 1 2 1 1 1 1 2 1 6 7 3 3 7 6 5 2 4 1\n",
            "9 3 1 3 7 6 9 6 6 7 3 3 1 1 1 6 6 1 1 1 3 3 7 6 6 9 6 7 3 1\n",
            "3 4 9 1 6 7 6 9 7 6 3 3 1 1 6 1 1 6 1 1 3 3 6 7 9 6 7 6 1 9\n",
            "7 5 4 1 1 3 4 1 2 1 1 1 5 5 2 5 5 2 5 5 1 1 1 2 1 4 3 1 1 4\n",
            "5 7 1 4 9 1 1 4 2 2 1 1 5 5 5 2 2 5 5 5 1 1 2 2 4 1 1 9 4 1\n",
            "4 1 9 9 4 3 1 3 1 1 1 6 2 5 5 5 5 5 5 2 6 1 1 1 3 1 3 4 9 9\n",
            "1 4 9 9 3 9 9 1 1 1 6 1 5 2 5 5 5 5 2 5 1 6 1 1 1 9 9 3 9 9\n",
            "7 6 6 6 9 6 1 3 1 3 4 1 9 6 7 6 6 7 6 9 1 4 3 1 3 1 6 9 6 6\n",
            "6 7 6 6 1 9 3 1 9 1 1 4 6 9 6 7 7 6 9 6 4 1 1 9 1 3 9 1 6 6\n",
            "6 6 7 6 1 3 9 1 9 3 1 3 7 6 5 2 2 5 6 7 3 1 3 9 1 9 3 1 6 7\n",
            "6 6 6 7 3 1 5 9 3 4 9 1 6 7 2 5 5 2 7 6 1 9 4 3 9 5 1 3 7 6\n",
            "9 1 1 3 1 7 6 5 9 9 4 1 1 3 4 1 1 4 3 1 1 4 9 9 5 6 7 1 3 1\n",
            "6 9 3 1 7 1 5 6 9 9 1 4 9 1 1 4 4 1 1 9 4 1 9 9 6 5 1 7 1 3\n",
            "\n",
            "Example 4 Solution:\n",
            "6 9 6 6\n",
            "9 3 9 9\n",
            "3 4 9 9\n",
            "1 9 4 1\n",
            "\n",
            "\n",
            "\n",
            "**TEST INPUT GRID:**\n",
            "\n",
            "4 4 1 3 5 7 7 9 6 1 6 6 4 4 7 7 7 7 4 4 6 6 1 6 9 7 7 5 3 1\n",
            "4 4 3 3 7 5 9 7 6 6 6 6 4 4 7 2 2 7 4 4 6 6 6 6 7 9 5 7 3 3\n",
            "3 4 4 4 7 9 5 7 5 1 6 1 7 7 9 9 9 9 7 7 1 6 1 5 7 5 9 7 4 4\n",
            "4 3 4 4 9 7 7 5 1 5 6 6 7 2 1 9 9 1 2 7 6 6 5 1 5 7 7 9 4 4\n",
            "9 7 7 4 4 4 3 3 4 4 7 7 9 7 3 2 2 3 7 9 7 7 4 4 3 3 4 4 4 7\n",
            "7 9 4 7 4 4 3 1 4 4 7 2 7 9 2 3 3 2 9 7 2 7 4 4 1 3 4 4 7 4\n",
            "7 4 9 7 3 4 4 4 7 7 9 1 7 4 9 7 7 9 4 7 1 9 7 7 4 4 4 3 7 9\n",
            "4 7 7 9 4 3 4 4 7 2 9 9 4 7 7 9 9 7 7 4 9 9 2 7 4 4 3 4 9 7\n",
            "6 6 5 1 4 4 7 7 7 2 2 6 4 6 2 2 2 2 6 4 6 2 2 7 7 7 4 4 1 5\n",
            "1 6 1 5 4 4 7 2 3 7 6 6 6 4 2 2 2 2 4 6 6 6 7 3 2 7 4 4 5 1\n",
            "6 6 6 6 7 7 9 9 9 1 7 2 2 2 4 6 6 4 2 2 2 7 1 9 9 9 7 7 6 6\n",
            "6 6 1 6 7 2 1 9 1 5 3 7 2 2 6 4 4 6 2 2 7 3 5 1 9 1 2 7 6 1\n",
            "4 4 7 7 9 7 7 4 9 9 1 6 7 2 6 6 6 6 2 7 6 1 9 9 4 7 7 9 7 7\n",
            "4 4 7 2 7 9 4 7 9 9 6 1 3 7 6 2 2 6 7 3 1 6 9 9 7 4 9 7 2 7\n",
            "8 8 8 1 3 2 9 7 1 6 9 9 5 1 7 2 2 7 1 5 9 9 6 1 7 9 2 3 1 9\n",
            "8 8 8 9 2 3 7 9 6 1 9 9 1 9 3 7 7 3 9 1 9 9 1 6 9 7 3 2 9 9\n",
            "8 8 8 9 2 3 7 9 6 1 9 9 1 9 3 7 7 3 9 1 9 9 1 6 9 7 3 2 9 9\n",
            "8 8 8 1 3 2 9 7 1 6 9 9 5 1 7 2 2 7 1 5 9 9 6 1 7 9 2 3 1 9\n",
            "8 8 8 2 7 9 4 7 9 9 6 1 3 7 6 2 2 6 7 3 1 6 9 9 7 4 9 7 2 7\n",
            "8 8 8 7 9 7 7 4 9 9 1 6 7 2 6 6 6 6 2 7 6 1 9 9 4 7 7 9 7 7\n",
            "8 8 8 6 7 2 1 9 1 5 3 7 2 2 6 4 4 6 2 2 7 3 5 1 9 1 2 7 6 1\n",
            "8 8 8 6 7 7 9 9 9 1 7 2 2 2 4 6 6 4 2 2 2 7 1 9 9 9 7 7 6 6\n",
            "8 8 8 5 4 4 7 2 3 7 6 6 6 4 2 2 2 2 4 6 6 6 7 3 2 7 4 4 5 1\n",
            "6 6 5 1 4 4 7 7 7 2 2 6 4 6 2 2 2 2 6 4 6 2 2 7 7 7 4 4 1 5\n",
            "4 7 7 9 4 3 4 4 7 2 9 9 4 7 7 9 9 7 7 4 9 9 2 7 4 4 3 4 9 7\n",
            "7 4 9 7 3 4 4 4 7 7 9 1 7 4 9 7 7 9 4 7 1 9 7 7 4 4 4 3 7 9\n",
            "7 9 4 7 4 4 3 1 4 4 7 2 7 9 2 3 3 2 9 7 2 7 4 4 1 3 4 4 7 4\n",
            "9 7 7 4 4 4 3 3 4 4 7 7 9 7 3 2 2 3 7 9 7 7 4 4 3 3 4 4 4 7\n",
            "4 3 4 4 9 7 7 5 1 5 6 6 7 2 1 9 9 1 2 7 6 6 5 1 5 7 7 9 4 4\n",
            "3 4 4 4 7 9 5 7 5 1 6 1 7 7 9 9 9 9 7 7 1 6 1 5 7 5 9 7 4 4\n",
            "\n",
            "\n",
            "---------------------------\n",
            "\n",
            "Sending request to model: gpt-4o...\n",
            "Response received.\n",
            "\n",
            "--- Parsing LLM Response ---\n",
            "Parsing Succeeded.\n",
            "\n",
            "\u001b[91mFailure: Shape Mismatch ((7, 7) vs (9, 3))\u001b[0m\n",
            "\n",
            "--- Cell 8 Function Execution Summary ---\n",
            "Task ID: 0934a4d8\n",
            "Test Index: 0\n",
            "API/Parse/Compare Failed Critically: False\n",
            "Prediction Correct: False\n",
            "Failure Reason: Shape Mismatch ((7, 7) vs (9, 3))\n",
            "\n",
            "--- Cell 8 Raw LLM Response ---\n",
            "```\n",
            "To solve the problems, let's break down the analysis:\n",
            "\n",
            "### Rule Deduction:\n",
            "\n",
            "1. **Grid Analysis**:\n",
            "   - Each input grid is a 30x30 matrix.\n",
            "   - Each output grid seems to be smaller segments of the input, possibly extracted from specific regions.\n",
            "\n",
            "2. **Pattern Identification**:\n",
            "   - There's a recurring pattern in all transformations where the output appears extracted from two main areas of the input grid.\n",
            "   - These areas are often centrally located sections with more concentrated borders.\n",
            "\n",
            "3. **Specific Rule Extraction**:\n",
            "   - **Extract Subsection**: Identify two leading sections extracted from the input grid and form the output grid.\n",
            "   - From our pattern check:\n",
            "     - For Example 1: The central concentration of numbers with specific diagonally opposite directions forms a 9x4 grid section.\n",
            "     - For Example 2: The center-top section is 4x5.\n",
            "     - For Example 3: The center-edge concentrated section is 3x7.\n",
            "     - For Example 4: A central-end section forms a 4x4 grid.\n",
            "\n",
            "   **Output Construction**:\n",
            "   - The configuration of numbers and their size are specific to individual problem examples.\n",
            "   - The output grid is an extracted subsection of 3 to 9 rows by 3 to 9 columns, depending on the example.\n",
            "  \n",
            "  4. **Output Validation Through Examples**:\n",
            "    - All successful transformations align with specific regions extracted as indicated from above rules per grid.\n",
            "\n",
            "### Solve the Test Input:\n",
            "\n",
            "1. **Find Subsections** - Identifying the regions that align with the transposition of sizes:\n",
            "   - Based on our rule of selecting the central part:\n",
            "   - Identify a central concentration by the offsets as done earlier:\n",
            "     - Given alignments look for relevant sections yielding consistent results.\n",
            "\n",
            "2. **Determine output**: \n",
            "   - For this 30x30 grid, to construct a subsection of significant numbers, find the central block which likely depicts relevant grouping.\n",
            "\n",
            "3. **Observative extraction** - Rely on fixed smaller sections analogous to those in our training examples:\n",
            "   - Construct the 7x7 grid from a mid-area focusing on similar patterns.\n",
            "\n",
            "### Application on Test Grid:\n",
            "\n",
            "```json\n",
            "[\n",
            "  [6, 1, 6, 9, 9, 7, 7],\n",
            "  [3, 7, 6, 2, 5, 1, 9],\n",
            "  [5, 1, 9, 9, 1, 6, 1],\n",
            "  [9, 9, 5, 7, 3, 2, 9],\n",
            "  [9, 9, 5, 1, 7, 7, 3],\n",
            "  [5, 7, 3, 6, 6, 4, 2],\n",
            "  [1, 6, 9, 9, 1, 3, 7]\n",
            "]\n",
            "```\n",
            "\n",
            "We precisely follow through deduction based upon examples trained to interpret the pattern induced to target the grid in a test setting, delivering the desired extracted segment.\n",
            "```\n",
            "\n",
            "--- Cell 8 Visualization ---\n",
            "\n",
            "Ground Truth Visualization:\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJwAAAHgCAYAAADg9nfBAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcltJREFUeJzt3Xucz3X+///7u5hhHMbkleAEAAAAAACAoBJRFEXFPQgAAAAAAAAcPPiGEwAAAAAAAIJiwQkAAAAAAABBseAEAAAAAACAoFhwAgAAAAAAQFAsOAEAAAAAACAoFpwAAAAAAAAQFAtOAAAAAAAACIoFJwAAAAAAAATFghMAAAAAAACC+v8ACTBb0YJWCJAAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1250x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LLM Prediction Visualization (Test 0):\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJwAAAHfCAYAAAAV2sK5AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdwFJREFUeJzt3XuczeX+///nEoZxmnEq9HWLFghMAAAAAAAAixYITAAAAAAAAIsWCEwAAAAAAACJVvLAHAAAAAABAUdZtSWqBP8f2CVMKtP8ew/oWaP+SNEYF/xwD0t4o8OdotL11gfaf2XZNgfYfFt9wAgAAAAAAQKRYcAIAAAAAAECkWHACAAAAAABApFhwAgAAAAAAQKRYcAIAAAAAAECkikxK3fDG9SLrK31FdBXfU6OuTt84uq7Ss6PrCwAAAAAAFB18wwkAAAAAAACRYsEJAAAAAAAAkWLBCQAAAAAAAJFiwQkAAAAAAACRYsEJAAAAAAAAkWLBCQAAAAAAAJFiwQkAAAAAAACRYsEJAAAAAAAAkWLBCQAAAAAAAJFiwQkAAAAAAACRYsEJAAAAAAAAkWLBCQAAAAAAAJFiwQkAAAAAAACRYsEJAAAAAAAAkWLBCQAAAAAAAJFiwQkAAAAAAACRYsEJAAAAAAAAkWLBCQAAAAAAAJGKBUEQFPYgAAAAAAAAcOzgG04AAAAAAACIFAtOAAAAAAAAiBQLTgAAAAAAAIgUC04AAAAAAACIFAtOAAAAAAAAiBQLTgAAAAAAAIgUC04AAAAAAACIFAtOAAAAAAAAiBQLTgAAAAAAAIjU/wfdBcTkUaTOdAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1250x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# CELL 8 Code\n",
        "\n",
        "import copy # Needed for deepcopy\n",
        "\n",
        "# --- Function to process and evaluate a single test case ---\n",
        "def run_and_evaluate_single_test_case(\n",
        "    task_id: str,\n",
        "    test_index: int,\n",
        "    all_tasks: Dict[str, TaskData],\n",
        "    model_name: str,\n",
        "    system_prompt: str,\n",
        "    user_prompt_template: str,\n",
        "    print_details: bool = True # Control printing during execution\n",
        ") -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Runs the LLM solving process for a single test case, optionally prints details,\n",
        "    and returns a detailed result summary including failure reasons.\n",
        "    Does NOT visualize. Returns data needed for external visualization.\n",
        "    \"\"\"\n",
        "    # Basic check if task exists\n",
        "    if task_id not in all_tasks:\n",
        "        if print_details: print(f\"\\033[91mError: Task ID '{task_id}' not found.\\033[0m\")\n",
        "        return None\n",
        "\n",
        "    # Initialize results\n",
        "    parsed_predicted_grid = None\n",
        "    raw_llm_response = None\n",
        "    is_correct = False\n",
        "    api_or_parse_failed = True # Assume critical failure initially\n",
        "    failure_reason = \"Task Data Error\"\n",
        "    task_data_dict = None\n",
        "    ground_truth_output_single = None\n",
        "    messages = None # Initialize messages\n",
        "\n",
        "    # 1. Get Task Data Components\n",
        "    try:\n",
        "        task_data_dict = all_tasks[task_id]\n",
        "        train_pairs = get_train_pairs(task_id, all_tasks)\n",
        "        test_inputs = get_test_inputs(task_id, all_tasks)\n",
        "        test_outputs_truth = get_test_outputs(task_id, all_tasks) # List of all ground truths\n",
        "    except Exception as e:\n",
        "         if print_details: print(f\"\\033[91mError accessing task data for '{task_id}': {e}.\\033[0m\")\n",
        "         return { # Return minimal failure info\n",
        "            \"task_id\": task_id, \"test_index\": test_index, \"predicted_grid\": None,\n",
        "            \"ground_truth\": None, \"is_correct\": False, \"api_or_parse_failed\": True,\n",
        "            \"failure_reason\": failure_reason, \"raw_response\": None, \"task_data_dict\": None,\n",
        "            \"messages\": None\n",
        "        }\n",
        "\n",
        "    # Check if the test case index is valid\n",
        "    if test_index >= len(test_inputs) or test_index >= len(test_outputs_truth):\n",
        "        failure_reason = f\"Invalid Test Index ({test_index}/{len(test_inputs)})\"\n",
        "        if print_details: print(f\"\\033[91mError: {failure_reason} for task '{task_id}'.\\033[0m\")\n",
        "        api_or_parse_failed = True\n",
        "        is_correct = False\n",
        "    else:\n",
        "        target_test_input = test_inputs[test_index]\n",
        "        ground_truth_output_single = test_outputs_truth[test_index]\n",
        "        failure_reason = None # Reset failure reason\n",
        "\n",
        "        # 2. Prepare Prompt Messages\n",
        "        messages = prepare_prompt_messages(\n",
        "            system_template=system_prompt,\n",
        "            user_template=user_prompt_template,\n",
        "            train_pairs=train_pairs,\n",
        "            test_input_grid=target_test_input\n",
        "        )\n",
        "\n",
        "        if print_details:\n",
        "            print(\"\\n--- Messages Sent to LLM --- \")\n",
        "            print(\"System Message:\")\n",
        "            print(messages[0]['content'])\n",
        "            print(\"\\nUser Message:\")\n",
        "            print(messages[1]['content'])\n",
        "            print(\"---------------------------\")\n",
        "\n",
        "        # 3. Get LLM Response\n",
        "        if print_details: print(f\"\\nSending request to model: {model_name}...\")\n",
        "        raw_llm_response = get_llm_response(messages, model_name)\n",
        "\n",
        "        if raw_llm_response is None:\n",
        "            failure_reason = \"No LLM Response\"\n",
        "            api_or_parse_failed = True\n",
        "            is_correct = False\n",
        "            if print_details: print(f\"\\033[91mFailure: {failure_reason}\\033[0m\")\n",
        "        else:\n",
        "            if print_details: print(\"Response received.\")\n",
        "            # 4. Parse the LLM Response\n",
        "            if print_details: print(\"\\n--- Parsing LLM Response ---\")\n",
        "            parsed_predicted_grid = parse_llm_response_for_grid(raw_llm_response)\n",
        "\n",
        "            if parsed_predicted_grid is None:\n",
        "                failure_reason = \"Parsing Failed\"\n",
        "                api_or_parse_failed = True\n",
        "                is_correct = False\n",
        "                if print_details: print(f\"\\033[91mFailure: {failure_reason}\\033[0m\")\n",
        "            else:\n",
        "                 api_or_parse_failed = False # Got a grid\n",
        "                 if print_details: print(\"Parsing Succeeded.\")\n",
        "\n",
        "                 # 6. Compare parsed grid with ground truth\n",
        "                 try:\n",
        "                     np_predicted = np.array(parsed_predicted_grid)\n",
        "                     np_truth = np.array(ground_truth_output_single)\n",
        "                     if np_predicted.shape != np_truth.shape:\n",
        "                          failure_reason = f\"Shape Mismatch ({np_predicted.shape} vs {np_truth.shape})\"\n",
        "                          is_correct = False\n",
        "                          if print_details: print(f\"\\n\\033[91mFailure: {failure_reason}\\033[0m\")\n",
        "                     elif not np.array_equal(np_predicted, np_truth):\n",
        "                          failure_reason = \"Content Mismatch\"\n",
        "                          is_correct = False\n",
        "                          if print_details: print(f\"\\n\\033[91mFailure: {failure_reason}\\033[0m\")\n",
        "                     else:\n",
        "                          failure_reason = None # Success\n",
        "                          is_correct = True\n",
        "                          if print_details: print(\"\\n\\033[92mMatch: Prediction matches ground truth!\\\\033[0m\")\n",
        "                 except ValueError as e:\n",
        "                      failure_reason = \"Comparison Error\"\n",
        "                      api_or_parse_failed = True # Treat comparison error as critical\n",
        "                      is_correct = False\n",
        "                      if print_details: print(f\"\\n\\033[91mFailure: {failure_reason}: {e}\\033[0m\")\n",
        "\n",
        "    # --- Return results ---\n",
        "    # Ground truth might be None if index was invalid\n",
        "    gt_output = ground_truth_output_single\n",
        "\n",
        "    return {\n",
        "        \"task_id\": task_id,\n",
        "        \"test_index\": test_index,\n",
        "        \"predicted_grid\": parsed_predicted_grid,\n",
        "        \"ground_truth\": gt_output,\n",
        "        \"is_correct\": is_correct,\n",
        "        \"api_or_parse_failed\": api_or_parse_failed,\n",
        "        \"failure_reason\": failure_reason,\n",
        "        \"raw_response\": raw_llm_response,\n",
        "        \"task_data_dict\": task_data_dict, # Needed for visualization\n",
        "        \"messages\": messages # Optionally return messages for debugging\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Configuration and Test Execution for Cell 8 ---\n",
        "EXAMPLE_TASK_ID_CELL8 = '0934a4d8'\n",
        "TEST_CASE_INDEX_CELL8 = 0\n",
        "MODEL_NAME_CELL8 = 'gpt-4o'\n",
        "\n",
        "if all_task_data:\n",
        "     # Call the function to run the example WITH printing details\n",
        "     result_summary_cell8 = run_and_evaluate_single_test_case(\n",
        "         task_id=EXAMPLE_TASK_ID_CELL8,\n",
        "         test_index=TEST_CASE_INDEX_CELL8,\n",
        "         all_tasks=all_task_data,\n",
        "         model_name=MODEL_NAME_CELL8,\n",
        "         system_prompt=SYSTEM_PROMPT_TEMPLATE,\n",
        "         user_prompt_template=USER_PROMPT_TEMPLATE,\n",
        "         print_details=True # Enable printing for single test\n",
        "     )\n",
        "\n",
        "     print(\"\\n--- Cell 8 Function Execution Summary ---\")\n",
        "     if result_summary_cell8:\n",
        "         print(f\"Task ID: {result_summary_cell8['task_id']}\")\n",
        "         print(f\"Test Index: {result_summary_cell8['test_index']}\")\n",
        "         print(f\"API/Parse/Compare Failed Critically: {result_summary_cell8['api_or_parse_failed']}\")\n",
        "         print(f\"Prediction Correct: {result_summary_cell8['is_correct']}\")\n",
        "         print(f\"Failure Reason: {result_summary_cell8['failure_reason']}\")\n",
        "\n",
        "         # Print full LLM response BEFORE visualization\n",
        "         print(\"\\n--- Cell 8 Raw LLM Response ---\")\n",
        "         print(f\"```\\n{result_summary_cell8['raw_response']}\\n```\" if result_summary_cell8['raw_response'] else \"Raw LLM Response: None\")\n",
        "\n",
        "         # Visualize AFTER function call and printing response\n",
        "         print(\"\\n--- Cell 8 Visualization ---\")\n",
        "         if result_summary_cell8['task_data_dict'] and result_summary_cell8['ground_truth'] is not None:\n",
        "              task_outputs_truth_cell8 = get_test_outputs(result_summary_cell8['task_id'], all_task_data)\n",
        "              print(f\"\\nGround Truth Visualization:\")\n",
        "              visualize_task(result_summary_cell8['task_data_dict'],\n",
        "                             task_solutions=task_outputs_truth_cell8,\n",
        "                             title=f\"Task: {result_summary_cell8['task_id']} (Ground Truth)\")\n",
        "\n",
        "              # Visualize prediction or print failure reason\n",
        "              if result_summary_cell8['predicted_grid'] is not None:\n",
        "                   print(f\"\\nLLM Prediction Visualization (Test {result_summary_cell8['test_index']}):\")\n",
        "                   solutions_llm_viz_cell8 = copy.deepcopy(task_outputs_truth_cell8)\n",
        "                   # Ensure index is valid before assignment (should be due to checks in function)\n",
        "                   if result_summary_cell8['test_index'] < len(solutions_llm_viz_cell8):\n",
        "                       solutions_llm_viz_cell8[result_summary_cell8['test_index']] = result_summary_cell8['predicted_grid']\n",
        "                   visualize_task(result_summary_cell8['task_data_dict'],\n",
        "                                  task_solutions=solutions_llm_viz_cell8,\n",
        "                                  title=f\"Task: {result_summary_cell8['task_id']} (LLM Pred for Test {result_summary_cell8['test_index']+1})\")\n",
        "              else:\n",
        "                   print(f\"\\nLLM Prediction Visualization skipped. Reason: {result_summary_cell8['failure_reason']}\")\n",
        "         else:\n",
        "              print(\"\\nVisualization skipped (Task data or ground truth missing/invalid index).\")\n",
        "\n",
        "     else:\n",
        "         print(\"Function execution failed or returned None.\")\n",
        "else:\n",
        "     print(\"\\nNo task data loaded, skipping function test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Benchmark: Run All Loaded Tasks\n",
        "\n",
        "Runs the LLM solver on all loaded tasks concurrently. Uses the grid string format for input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Benchmark ---\n",
            "Model: gpt-4o\n",
            "Max Concurrent Requests: 50\n",
            "Tasks to process: 120\n",
            "Total test cases to process: 172\n",
            "Progress: 172/172 test cases completed.\n",
            "Progress: 172/172 test cases completed.\n",
            "Benchmark finished in 153.63 seconds.\n",
            "\n",
            "--- Benchmark Final Summary ---\n",
            "Tasks Attempted: 120\n",
            "Total Test Cases Processed: 172\n",
            "Test Cases Correctly Solved: 0\n",
            "Test Cases Failed (API/Parse/Compare Error): 24\n",
            "Test Cases Incorrect (Shape/Content Mismatch): 148\n",
            "\n",
            "Test Case Success Rate (Correct / Successfully Processed): 0.00%\n",
            "Overall Test Case Accuracy (Correct / Total): 0.00%\n",
            "\n",
            "Tasks Solved (All Test Cases Correct & Processed): 0\n",
            "Task Success Rate: 0.00%\n",
            "\n",
            "Failure Reason Distribution:\n",
            "- Comparison Error: 9\n",
            "- Content Mismatch: 79\n",
            "- Parsing Failed: 15\n",
            "- Shape Mismatch ((10, 10) vs (13, 13)): 1\n",
            "- Shape Mismatch ((10, 8) vs (15, 15)): 1\n",
            "- Shape Mismatch ((10, 8) vs (20, 8)): 1\n",
            "- Shape Mismatch ((11, 11) vs (14, 11)): 1\n",
            "- Shape Mismatch ((11, 11) vs (5, 23)): 1\n",
            "- Shape Mismatch ((11, 7) vs (19, 7)): 1\n",
            "- Shape Mismatch ((12, 20) vs (16, 16)): 1\n",
            "- Shape Mismatch ((12, 6) vs (18, 8)): 1\n",
            "- Shape Mismatch ((12, 8) vs (16, 8)): 1\n",
            "- Shape Mismatch ((13, 10) vs (18, 13)): 1\n",
            "- Shape Mismatch ((13, 13) vs (7, 30)): 1\n",
            "- Shape Mismatch ((13, 25) vs (26, 26)): 1\n",
            "- Shape Mismatch ((14, 16) vs (30, 16)): 1\n",
            "- Shape Mismatch ((15, 6) vs (15, 15)): 1\n",
            "- Shape Mismatch ((16, 13) vs (19, 13)): 1\n",
            "- Shape Mismatch ((16, 16) vs (14, 21)): 1\n",
            "- Shape Mismatch ((16, 30) vs (25, 25)): 1\n",
            "- Shape Mismatch ((16, 5) vs (8, 17)): 1\n",
            "- Shape Mismatch ((17, 17) vs (19, 17)): 1\n",
            "- Shape Mismatch ((18, 22) vs (30, 22)): 1\n",
            "- Shape Mismatch ((18, 28) vs (19, 28)): 1\n",
            "- Shape Mismatch ((19, 16) vs (29, 16)): 1\n",
            "- Shape Mismatch ((19, 20) vs (20, 20)): 1\n",
            "- Shape Mismatch ((19, 24) vs (24, 24)): 1\n",
            "- Shape Mismatch ((19, 7) vs (25, 7)): 1\n",
            "- Shape Mismatch ((20, 21) vs (22, 21)): 1\n",
            "- Shape Mismatch ((20, 8) vs (14, 14)): 1\n",
            "- Shape Mismatch ((21, 15) vs (30, 20)): 1\n",
            "- Shape Mismatch ((22, 6) vs (14, 11)): 1\n",
            "- Shape Mismatch ((23, 20) vs (21, 21)): 1\n",
            "- Shape Mismatch ((23, 26) vs (24, 26)): 1\n",
            "- Shape Mismatch ((24, 18) vs (25, 18)): 1\n",
            "- Shape Mismatch ((24, 21) vs (25, 21)): 1\n",
            "- Shape Mismatch ((25, 26) vs (26, 26)): 3\n",
            "- Shape Mismatch ((25, 7) vs (10, 25)): 1\n",
            "- Shape Mismatch ((25, 7) vs (29, 8)): 1\n",
            "- Shape Mismatch ((26, 25) vs (30, 25)): 1\n",
            "- Shape Mismatch ((26, 27) vs (27, 27)): 2\n",
            "- Shape Mismatch ((26, 30) vs (30, 30)): 1\n",
            "- Shape Mismatch ((27, 28) vs (28, 28)): 1\n",
            "- Shape Mismatch ((27, 30) vs (30, 30)): 2\n",
            "- Shape Mismatch ((28, 29) vs (29, 29)): 1\n",
            "- Shape Mismatch ((28, 30) vs (30, 30)): 2\n",
            "- Shape Mismatch ((29, 28) vs (30, 28)): 1\n",
            "- Shape Mismatch ((3, 7) vs (19, 5)): 1\n",
            "- Shape Mismatch ((30, 8) vs (30, 22)): 1\n",
            "- Shape Mismatch ((31, 30) vs (30, 30)): 1\n",
            "- Shape Mismatch ((32, 30) vs (30, 30)): 1\n",
            "- Shape Mismatch ((4, 15) vs (5, 29)): 1\n",
            "- Shape Mismatch ((4, 4) vs (6, 6)): 1\n",
            "- Shape Mismatch ((5, 3) vs (5, 6)): 1\n",
            "- Shape Mismatch ((5, 5) vs (20, 12)): 1\n",
            "- Shape Mismatch ((5, 5) vs (8, 5)): 1\n",
            "- Shape Mismatch ((5, 5) vs (9, 14)): 1\n",
            "- Shape Mismatch ((6, 8) vs (8, 10)): 1\n",
            "- Shape Mismatch ((7, 22) vs (14, 19)): 1\n",
            "- Shape Mismatch ((7, 3) vs (8, 8)): 1\n",
            "- Shape Mismatch ((7, 7) vs (9, 11)): 1\n",
            "- Shape Mismatch ((7, 7) vs (9, 13)): 1\n",
            "- Shape Mismatch ((8, 12) vs (12, 12)): 1\n",
            "- Shape Mismatch ((8, 4) vs (10, 8)): 1\n",
            "- Shape Mismatch ((8, 8) vs (30, 30)): 1\n",
            "- Shape Mismatch ((9, 12) vs (24, 24)): 1\n",
            "- Shape Mismatch ((9, 9) vs (5, 10)): 1\n",
            "\n",
            "\n",
            "--- Detailed Results & Visualization Phase ---\n",
            "\n",
            "\n",
            "===== Task: 0934a4d8 / Test Case: 0 =====\n",
            "\n",
            "--- Raw LLM Response ---\n",
            "```\n",
            "To transform the input grid into the desired output grid, we need to deduce the rules applied during the transformation of the input to the output in the previously seen examples. \n",
            "\n",
            "Upon examining the given examples, here are some observations:\n",
            "\n",
            "1. In each example's solution, specific patterns of numbers are extracted from the given input grid, potentially concentrating on certain quadrants or characteristic sections.\n",
            "2. The result is typically a much smaller grid, implying significant reduction from the original input grid.\n",
            "3. The output grid appears to display certain symmetrical patterns or central areas from the input grid.\n",
            "\n",
            "Next, let's deduce the rules from the provided train examples:\n",
            "\n",
            "### Deduction Process:\n",
            "1. **Grid Reduction:** The output grid is a smaller subset of the original grid. It's likely focusing on a particular pattern or repeated area within the input grid. \n",
            "2. **Focus Region:** Observing the focused numbers in the output, it seems they emphasize high-frequency patterns or distinct blocks within certain rows or columns of the input grid.\n",
            "3. **Consistent Extraction:** The numbers often extracted are from areas resembling condensed repetitions of numbers or altered blocks within sub-regions of the input.\n",
            "4. **Symmetry and Centralization:** The regions chosen reflect angular symmetry or prominent features like central areas or corners of the concentrated grid.\n",
            "5. **Size Consistency:** The size of the solution grids across examples remains relatively the same.\n",
            "\n",
            "### Rules Found:\n",
            "- Extract specific sections or subarrays from the input grid that appear as dense or structured blocks of numbers.\n",
            "- These regions are frequently part of larger consistent regions in the grid, like centric blocks, vertical symmetry lines, or dense number zones.\n",
            "  \n",
            "Applying these deductions to our test input grid:\n",
            "\n",
            "```\n",
            "8 8 8 9\n",
            "9 9 1 6\n",
            "3 3 6 6\n",
            "9 9 1 6\n",
            "7 6 6 6\n",
            "``` \n",
            "\n",
            "Following these rules observed from examples, the above output is created. Each train and test input follows a similar pattern of extracting symmetrical dense sections or blocks of numbers in the form of subarrays from the grid. If any asymmetry or randomness in numbers exists, it consistently reduces focus into specific key repeated sections or structures. \n",
            "\n",
            "Thus, the output transforms large inputs into concentrated quadrants illustrating dense repetitions pertinent to visual blocks observed in each input example provided.\n",
            "```\n",
            "\n",
            "--- Ground Truth Visualization (Task: 0934a4d8) ---\n"
          ]
        },