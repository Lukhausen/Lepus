#import "@preview/supercharged-dhbw:3.4.0": acr, acrf
#import "../utils/llm.typ": llm-input, llm-output, llm-interaction

#let introduction = [

= Introduction <into>


Large language models (LLMs) have historically improved through scaling laws, where increases in parameters and training data correlate with enhanced performance following predictable power-law relationships. These scaling laws revealed that certain emergent abilities, such as solving unseen mathematical problems, would suddenly appear at specific parameter thresholds rather than developing gradually. However, performance gains began to plateau nonlinearly: doubling model size no longer doubled capability, while computational costs grew superlinearly following power-law relationships. @wei2022emergentabilitieslargelanguage This diminishing return from traditional scaling laws necessitated alternative strategies, leading to the exploration of test-time compute — enhancing reasoning during inference rather than solely relying on larger architectures.

Early LLMs (e.g., GPT-3) demonstrated that scaling parameters unlocked novel capabilities. However, as models grew beyond hundreds of billions of parameters, performance improvements became sublinear relative to resource investment, indicating fundamental limitations to the scaling paradigm.

In 2022, researchers at Google introduced chain-of-thought (CoT) prompting, enabling models to decompose problems into intermediate steps during inference. This method significantly improved performance on arithmetic, commonsense, and symbolic reasoning tasks. @wei2023chainofthoughtpromptingelicitsreasoning Crucially, CoT shifted computational burden to inference time, decoupling performance gains from model size alone and establishing a new direction for enhancing language model capabilities.

On May 31, 2023, OpenAI released PRM800K, a dataset of 800,000 human-curated step-by-step reasoning traces documenting mathematical problem-solving processes. @prm800k The underlying concept behind these human-generated chains of thought was to train a model to evaluate the accuracy of step-by-step reasoning chains produced by LLMs.

The accompanying paper details how this dataset was used to train a Process Reward Model (PRM), which evaluates intermediate reasoning steps generated by language models. This approach proved more effective than using an Outcome Reward Model (ORM), which only assessed the final result of a reasoning chain. @lightman2023letsverifystepstep

A particularly significant observation from this research was articulated in the following statement:

#quote(attribution: [@lightman2023letsverifystepstep], ["We do
not discuss any supervision the generator #footnote([The LLM that generated a reasoning chain, e.g., GPT-4]) would receive from the reward model
if trained with RL. Although finetuning the generator with RL is a natural next
step, it is intentionally not the focus of this work."])

This statement indicates the potential to fine-tune an LLM using this reward model to leverage chain-of-thought reasoning during inference. Approximately a year later, on September 12, 2024, OpenAI released o1 — the first LLM designed to incorporate chain-of-thought reasoning in its inference process by default. It demonstrated superior performance across mathematical and logical reasoning benchmarks at the time of its release. @openai2024o1systemcard

Despite these advances, o1 still exhibits limitations with certain reasoning tasks. For example:

#llm-interaction(
  model: "o1",
  [Beth places four whole ice cubes in a frying pan at the start of the first minute, then five at the start of the second minute and some more at the start of the third minute, but none in the fourth minute. If the average number of ice cubes per minute placed in the pan while it was frying a crispy egg was five, how many whole ice cubes can be found in the pan at the end of the third minute? @simplebench_try_yourself],
  [[...] Therefore, the number of whole ice cubes in the pan at the end of the third minute is 11 #footnote([The confusion of the model primarily stems from overlooking the contextual information about an egg frying in the pan, focusing instead on the mathematical components of the problem.])]
)

As OpenAI has transitioned from its original open-source mission to a for-profit enterprise, the methodologies behind o1's training remain proprietary. However, in January 2025, DeepSeek — a Chinese AI company — released DeepSeek-R1, an LLM demonstrating performance comparable to OpenAI's o1. DeepSeek made their model weights fully available for download and use, publishing a comprehensive paper detailing how they achieved this performance with minimal training costs. @deepseekai2025deepseekr1incentivizingreasoningcapability

The fundamental approach described in their paper employs a rule-based outcome reward function that verifies the correctness of final answers for reasoning-based questions in domains like mathematics and coding. This framework allows the model to develop its own approach to solution-finding through reinforcement learning. To encourage structured reasoning, the researchers rewarded the model not only for correct outputs but also for employing a specific output structure:
 $ "<think>...</think><answer>...</answer>" $

An intriguing development occurred during training — the model progressively increased its response length and exhibited emergent behaviors without explicit instruction. Notably, it began to self-reflect on previous steps and reevaluate its answers, despite not being exposed to such behaviors in the training data. This spontaneous development of meta-cognitive reasoning strategies was later termed the "aha moment" phenomenon. #footnote([Multiple post-training steps, fine-tuning, and even full retraining occurred in later stages, but for the sake of brevity we will not discuss this here]) In one documented instance, the model generated:

#llm-output(
  model: "deepseek-r1-zero",
  "[...] Wait, wait. Wait. That's an aha moment I can flag here. Let's reevaluate this step-by-step [...]"
)

Since this approach requires only input-output pairs as training data, we hypothesize that it could be applied to domains where LLMs have previously struggled to produce tangible results. One such domain is spatial generalization on novel tasks, as evaluated by the ARC-AGI-2-Benchmark. This benchmark consists of example grids that must be transformed into output grids, requiring models to learn from examples and generalize observed patterns to new tasks. Currently, the highest score on this benchmark is held by OpenAI's o3 model at 4%, compared to human performance of 100%. @arcprize_leaderboard

Our research investigates whether DeepSeek's reinforcement learning methodology can be adapted to enhance pattern generalization capabilities on the ARC-AGI-2 dataset, potentially bridging the substantial gap between current AI systems and human-level performance in abstract reasoning tasks.

Specifically, our approach involves: 
+ implementing a bifurcated reward function that evaluates structural and content components separately
+ training 3B and 7B parameter models using reinforcement learning with incentives for extended reasoning chains
+ testing the models' emergent reasoning capabilities
+ when natural emergence fails, strategically kickstarting reasoning development through targeted reward mechanisms that explicitly encourage longer reasoning patterns before transitioning to content-focused optimization. This methodology emphasizes the development of intermediate reasoning processes that more closely approximate human problem-solving strategies when addressing abstract reasoning challenges.

]


