#let glossary = (
  "Ablation Study": "Systematically removing system parts to evaluate their contribution.",
  "Activations": "The output value of a neuron or layer in a neural network.",
  "Attention Mechanism": "Neural network technique weighting input parts' importance for output generation.",
  "Backpropagation": "Algorithm for training neural networks by computing weight gradients.",
  "Checkpoint (Model)": "A saved model state (parameters, optimizer) allowing training resumption.",
  "Convergence": "Training state where model performance plateaus.",
  "Critic (in RL)": "RL component estimating state/action value to guide policy learning.",
  "Emergent Behaviors": "Unprogrammed capabilities arising from model scale or training dynamics.",
  "Ensemble Approach": "Combining predictions from multiple models to improve overall performance.",
  "Exploration (in RL)": "Agent trying varied actions to discover optimal strategies.",
  "Few-shot Prompting": "Guiding an LLM using a few examples within the prompt, without fine-tuning.",
  "Fine-tuning": "Adapting a pre-trained model to a specific task/dataset.",
  "Foundation Model": "Large, pre-trained model adaptable to various downstream tasks.",
  "Forward Pass": "Process of feeding input through network layers to generate an output.",
  "Gradient Checkpointing": "Memory-saving technique trading computation for memory by recomputing activations.",
  "Gradients": "Vectors indicating the direction/rate of loss change relative to model parameters.",
  "Greedy Stacking": "Heuristic combining partial solutions based on immediate best performance (context-specific).",
  "Hallucination (LLM)": "LLM generating plausible but factually incorrect or nonsensical output.",
  "Hidden State": "Internal memory in sequence models carrying information about past inputs.",
  "Hyperparameter Tuning": "Optimizing parameters set *before* training starts (e.g., learning rate).",
  "Inference": "Using a trained model to make predictions on new data; test time.",
  "KV Cache": "Caching attention keys/values during inference to accelerate generation.",
  "Local Minimum": "Suboptimal point in parameter space where training can get stuck.",
  "Meta-cognitive reasoning": "Model reasoning *about* its thinking process instead of the task.",
  "Normalization (Reward)": "Scaling RL rewards to a standard range to stabilize training.",
  "Optimizer States": "Internal variables (e.g., momentum) maintained by optimization algorithms.",
  "Parameter Space": "The multi-dimensional space defined by all adjustable model parameters.",
  "Recency Bias": "Disproportionate influence of recent information in a sequence.",
  "Regularisation": "Techniques (e.g., dropout) preventing overfitting by penalizing model complexity.",
  "Reward Clipping": "Limiting RL reward values to a specific range to stabilize training.",
  "Reward Hacking": "Agent maximizing reward in unintended ways, failing the actual task.",
  "Reward Saturation": "Rewards plateauing despite improvements in desired behavior.",
  "Scalarisation": "Combining multiple objectives or rewards into a single scalar value.",
  "Spatial Generalization": "Applying learned spatial patterns/rules to new, unseen spatial tasks.",
  "Spatial Reasoning": "Ability to understand and manipulate spatial relationships.",
  "Temperature": "Parameter controlling randomness/determinism in LLM generation.",
  "Tokenization": "Breaking input text/sequences into smaller units (tokens) for model processing.",
  "Transformer Architecture": "Dominant deep learning architecture for sequence tasks, using self-attention.",
  "Visual Reasoning Primitives": "Basic visual operations combined to solve complex visual tasks.",
)