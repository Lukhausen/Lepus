#import "@preview/supercharged-dhbw:3.4.0": acr, acrf
#import "../utils/llm.typ": llm-input, llm-output, llm-interaction

#let reward = [
  = Defining a Reward Function

Paramount to a successful training in RL is to have a reward function that actually reflects what we want to achieve in the RL training run. It's important to first of all achieve the foundational goals before heading to more advanced goals. In our case, one of the most foundational goals of the LLM is to produce algorithmically parsable output so we can even start to compare it to the actually desired result.
The paper 'Multi-Objective Reinforcement Learning from AI Feedback' @williams2024multiobjectivereinforcementlearningai (MORLAIF) provides valuable insights into the optimisation of AI evaluation systems by separating different evaluation aspects. These findings can be applied to the development of a two-stage evaluation structure that evaluates syntax and content agreement separately.
MORLAIF demonstrates that decomposing complex evaluation tasks into more specific subtasks leads to better overall results. Rather than using a single preference algorithm that covers all aspects of evaluation simultaneously, the paper shows clear advantages of developing separate models for different principles such as factuality, toxicity and comprehensibility.
This realisation can be applied directly to the evaluation of AI expenditure. Instead of using a single evaluation metric that covers all aspects, the separation into syntactic and content evaluation dimensions is logical and effective.
If you apply this to our goal, you get two evaluation aspects: firstly, compliance with the required list format and the correct grid dimensions, and secondly, compliance of the content result with the requirement.
The advantages of such a separate evaluation of syntax and content are confirmed by the MORLAIF paper: 'By breaking down preference modelling into specific principles, the feedback collection and preference model training becomes a more straightforward and well-defined task, which we hypothesize will lead to improved preference model performance.' This hypothesis was confirmed in the experiments, with the specific models achieving significantly higher accuracy than individual models.
We also use scalarisation functions to combine the separate ratings. These functions offer flexible methods for combining the two evaluation dimensions (syntax and content) into an overall evaluation. By merging them, we can then utilise and evaluate the reward score more efficiently.
The score management in our evaluation system is designed around the insight that separating evaluation into distinct facets—syntax and content—can lead to a more nuanced and effective reward mechanism for our RL training. Building on the MORLAIF-inspired architecture, we decompose the problem into two specific evaluations.
The first evaluation focuses on syntax (structural) evaluation. We first ensure that the LLM output adheres to the required formatting, namely producing a syntactically correct list or grid format. The function evaluate_grid_similarity is responsible for this aspect. Its key steps include parsing the input, where both the expected answer and the solution are parsed into Python objects using a safe literal evaluation method. If parsing fails or the structure does not match the expected format (i.e., a list of lists), a low baseline score (0.1) is assigned. The function also measures structural similarity by comparing the number of rows (using a row similarity ratio) and then iterates over the common rows to compute column-level similarity. For each row, the ratio of the lengths of the corresponding rows is computed. Finally, an exponential mapping is applied to the structural score to map it appropriately into a range that prevents the model from being over-rewarded before the result is fully accurate. This mapping ensures that the score for syntax is continuous and sensitive to even minor improvements, while capping it below a full score unless there's an exact match.
The second evaluation is content (semantic) evaluation. The function compare_answers is designed to assess the quality of the content produced by the LLM. It works through direct and fallback parsing, where the function attempts to parse both the expected and actual answers. If both are parsed successfully, it compares them directly. In cases where the exact match is not achieved, it "flattens" any nested lists to create a simple comparison baseline and then uses a similarity measure (via SequenceMatcher) to estimate how closely the content aligns. For scenarios where the solution cannot be parsed directly (possibly due to formatting issues), a regex-based fallback mechanism extracts numbers with regex and then computes a similarity ratio between the expected and provided values.
The decision to confine our reward score within the range of 0.1 to 0.9 for partial matches—with 0.1 representing the worst-case outcome and 0.9 representing nearly optimal performance, and a perfect score of 1.0 reserved only for an exact match—is driven by two key concepts: normalization/scaling and reward clipping.@10014846
== Normalization and Scaling:
In reinforcement learning, especially within complex multi-objective frameworks like those discussed in MORLAIF, it is crucial to ensure that the reward signal remains within a manageable and meaningful range. By normalizing our partial reward scores to lie between 0.1 and 0.9, we ensure that the signal is neither too weak nor excessively large. This scaling prevents issues such as reward saturation, where excessively high rewards may lead the model to overestimate the value of its predictions or cause instability during training. Inspired by approaches in modern RL systems—as also observed in studies on deep reinforcement learning for congestion control—the normalized range serves as a consistent baseline that fosters smoother gradient updates and more stable policy learning. Essentially, even when the model produces an output that is only partially correct, it still receives a non-zero reward (at least 0.1), which guarantees that the learning signal persists throughout the training process.
== Reward Clipping:
Reward clipping is another important mechanism that helps control the variance and stability of the training process. By capping partial rewards at 0.9, we deliberately prevent the model from receiving a near-perfect reward for outputs that are still not completely accurate. This technique mirrors the clipping practices observed in advanced RL algorithms like those implemented in PPO and as demonstrated in the ablation studies for DRL-based congestion control systems. In those studies, omitting clipping often led to erratic policy updates and convergence issues. Clipping ensures that while partial correctness is rewarded, it never reaches the level of an exact match. This careful capping of the reward avoids excessive optimism in the policy updates, ensuring that only the completely correct outputs garner the full reward of 1.0.
== Putting It All Together:
By setting the reward range from 0.1 to 0.9 for any output that is not an exact match, we integrate both normalization/scaling and clipping methods into our reward design. The scaling ensures that the model's learning dynamics remain stable and that the gradients are suitably informative even when outputs are only partially correct. Meanwhile, clipping keeps the reward signal bounded, which helps to prevent overshooting during policy updates and maintains a clear distinction between near-perfect performance (0.9) and absolute correctness (1.0).
This approach, inspired by the insights from MORLAIF and corroborated by empirical findings in related reinforcement learning ablation studies, ultimately leads to a more robust and efficient training process—one where the reward function accurately reflects progress toward both the foundational goal of producing algorithmically parsable output and the advanced goal of content accuracy.
Finally, the main function evaluate_score combines both dimensions using scalarisation. It applies a scalarisation function that immediately returns a perfect score (1.0) if the solution exactly matches the expected answer. Otherwise, it computes the final score as a weighted sum of the syntax score and the content score. By default, both aspects are weighted equally (0.5 each), but these weights can be adjusted to better reflect their relative importance in different contexts.
By managing the score with this bifurcated approach, our method encourages foundational correctness by prioritizing the creation of algorithmically parsable output, establishing a solid structural foundation before delving into more sophisticated content verification. It also improves accuracy by isolating the evaluation into more specific tasks, helping in pinpointing and rewarding improvements in both structure and semantic quality, akin to the benefits observed in the MORLAIF study. Additionally, it provides flexibility through the use of scalarisation functions, allowing us to finely tune the overall reward, so that improvements in either syntax or content result in a corresponding improvement in the final score, leading to more efficient and targeted model training. This careful separation and subsequent recombination of evaluation aspects not only mirrors the empirical findings of MORLAIF but also ensures that our reward function accurately reflects the foundational and advanced goals necessary for successful reinforcement learning training.

]