#import "@preview/supercharged-dhbw:3.4.0": acr, acrf
#import "../utils/llm.typ": llm-input, llm-output, llm-interaction

#let post_training_benchmark = [
= Benchmarking of the Trained Models

In this chapter, we outline the process and methodology for benchmarking our models, which have been refined using reinforcement learning techniques. The primary focus during the benchmarking phase is to evaluate the content of the generated answers—assessing their correctness and relevance—without the distraction of syntax evaluation, which was primarily handled during training.

== Overview of the Benchmarking Process
After completing the training phase, where the reward function evaluated both output syntax and content, our benchmarking process isolates the content evaluation to ensure that the models generate accurate and meaningful responses. While syntax was necessary during training to guide the models toward well-structured output, the ultimate performance metric rests on the correctness and appropriateness of the content itself.

To achieve this, we developed a dedicated Python script that builds on the content evaluation mechanism used in our reward function. This script calculates two distinct performance scores:

1. *Content Reward Score:* This metric leverages the same evaluation function defined in the "Defining Reward Function" chapter. It specifically assesses the content quality of the output without considering syntax. The only difference is that we don't use a linear function as in the actual reward function. Instead, we output the ratio value directly without incorporating it into an e-function. By using this score, we can directly compare the models and see more indeph learning improvments.
2. *Correctness Percentage:* Independently, the script checks each response to determine whether it is factually correct or incorrect. By tallying the number of correct responses and dividing by the total number of tests, we obtain a percentage score that reflects the overall accuracy of the model.

== Methodology and Data Set
For benchmarking, we employ a separate data set that was specifically curated and adapted for our purposes. Our source for this test data is the ARC-AGI-2 @arcagi2025evaluation test set, which was provided as an initial collection of examples. Recognizing that direct application might not align perfectly with our unique requirements, we made several modifications. These adjustments ensured that the test set effectively challenges the models in areas crucial to our application domain.\
Our methodology follows these steps:

1. *Test Set Adaptation:* The downloaded ARC-AGI-2 data was reformatted to facilitate easier processing within our evaluation pipeline. This adaptation process involved only structural changes to the data format, without altering the content or examples themselves. The reformatting allowed us to feed the test data directly to our models without requiring additional processing steps during the evaluation.
2. *Content Evaluation:* Using the Python script, each output generated by the models is examined against predetermined content criteria. The reward score derived from this evaluation provides insight into how well the model understands and correctly conveys the intended information.
3. *Correctness Verification:* In parallel to content scoring, our evaluation mechanism categorizes each response as either correct or incorrect. This binary classification is then used to compute a percentage of correct answers, offering a straightforward metric of accuracy.
4. *Score Aggregation:* The final output of the benchmarking process includes both the content reward score and the correctness percentage as seen in @exmaple-out-eval These combined metrics offer a comprehensive view of the model's performance, highlighting strengths in content quality and areas that may need further refinement. 

#figure(
align(center)[
 #block(
 width: auto,
 inset: 10pt,
 fill: luma(240),
 radius: 4pt,
 align(left)[ ```
arc-agi-lepus-v1-evaluation
Correct: 0
Wrong: 120
Score: 0.0
RewardScore: 18.30003843640773
 ```
 ]
 )
], caption: "Example benchmark output for the arc-agi-lepus-v1-evaluation model"
) <exmaple-out-eval>

== Conclusion
The benchmarking phase serves as a critical step in validating the effectiveness of our reinforcement learning approach. By isolating the evaluation of content from syntax and concentrating on the factual correctness of the output, we ensure that the models are not only well-trained in generating fluent language but also in delivering reliable and relevant information.

The dual metrics—a detailed content reward score and a clear percentage of correct responses—provide a robust framework for continuous evaluation and future improvements. This structured approach to benchmarking not only confirms the current state of our models but also lays the groundwork for ongoing optimizations in response to real-world challenges.
]