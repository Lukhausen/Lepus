#import "@preview/supercharged-dhbw:3.4.0": acr, acrf
#import "../utils/llm.typ": llm-input, llm-output, llm-interaction

#let post_training_benchmark = [
= Benchmarking of the Trained Models

In this chapter, we outline the process and methodology for benchmarking our models, which have been refined using reinforcement learning techniques. The primary focus during the benchmarking phase is to evaluate the content of the generated answers—assessing their correctness and relevance—without the distraction of syntax evaluation, which was primarily handled during training.

== Overview of the Benchmarking Process
After completing the training phase, where the reward function evaluated both output syntax and content, our benchmarking process isolates the content evaluation to ensure that the models generate accurate and meaningful responses. While syntax was necessary during training to guide the models toward well-structured output, the ultimate performance metric rests on the correctness and appropriateness of the content itself.

To achieve this, we developed a dedicated Python script that builds on the content evaluation mechanism used in our reward function. This script calculates two distinct performance scores:

1. *Content Reward Score:* This metric leverages the same evaluation function defined in the "Defining Reward Function" chapter. It specifically assesses the content quality of the output without considering syntax. The only difference is that we don't use a linear function as in the actual reward function. Instead, we output the ratio value directly without incorporating it into an e-function. By using this score, we can directly compare the models and see more indeph learning improvments.
2. *Correctness Percentage:* Independently, the script checks each response to determine whether it is factually correct or incorrect. By tallying the number of correct responses and dividing by the total number of tests, we obtain a percentage score that reflects the overall accuracy of the model.

== Methodology and Data Set
For benchmarking, we employ a separate data set that was specifically curated and adapted for our purposes. Our source for this test data is the ARC-AGI-2 @arcagi2025evaluation test set, which was provided as an initial collection of examples. Recognizing that direct application might not align perfectly with our unique requirements, we made several modifications. These adjustments ensured that the test set effectively challenges the models in areas crucial to our application domain.\
Our methodology follows these steps:

1. *Test Set Adaptation:* The downloaded ARC-AGI-2 data was reformatted to facilitate easier processing within our evaluation pipeline. This adaptation process involved only structural changes to the data format, without altering the content or examples themselves. The reformatting allowed us to feed the test data directly to our models without requiring additional processing steps during the evaluation.
2. *Content Evaluation:* Using the Python script, each output generated by the models is examined against predetermined content criteria. The reward score derived from this evaluation provides insight into how well the model understands and correctly conveys the intended information.
3. *Correctness Verification:* In parallel to content scoring, our evaluation mechanism categorizes each response as either correct or incorrect. This binary classification is then used to compute a percentage of correct answers, offering a straightforward metric of accuracy.
4. *Score Aggregation:* The final output of the benchmarking process includes both the content reward score and the correctness percentage. These combined metrics offer a comprehensive view of the model's performance, highlighting strengths in content quality and areas that may need further refinement. 

== Comparison of Base and Thinking Models
In addition to the overall benchmarking process described earlier, we conducted a direct comparison between our base model (Qwen_2.5_7B) and the thinking-enhanced variant (Qwen_2.5_7B_ARC_v0.2_thinking). The base model, which served as our starting point, was trained without any additional incentives for extended reasoning. Its final performance on the test set was characterized by a RewardScore of approximately 0.12946 as seen in @exmaple-out-qwen_normal, even though both models registered zero correct outputs and 120 wrong outputs in terms of basic correctness.

#figure(
align(center)[
 #block(
 width: auto,
 inset: 10pt,
 fill: luma(240),
 radius: 4pt,
 align(left)[ ```
  Correct: 0
  Wrong: 120
  Score: 0.0
  RewardScore: 0.12946181379318953
 ```
 ]
 )
], caption: "Benchmark output for the Qwen_2.5_7B model"
) <exmaple-out-qwen_normal>

The thinking model, on the other hand, initially underwent the same standard training process as the base model. However, upon reaching a plateau in its reasoning development, we introduced a targeted intervention. Specifically, we provided a reward incentive for longer, more elaborate reasoning processes. This "kickstart" encouraged the model to develop a more in-depth thought process in its outputs. Once the model had sufficiently adopted this longer reasoning approach, we removed the extra reward and reverted to training exclusively with the standard RewardScore. The goal was to integrate the enhanced thinking capability with the model’s ability to generate correct and contextually relevant solutions.

The results clearly show that the thinking model outperforms the base model in terms of the RewardScore, as evidenced by an increase from around 0.12946 to roughly 0.22992 as seen in @exmaple-out-qwen_rhinking. This improvement indicates that the additional reasoning component—enhanced during the specialized training phase—has a positive effect on the content quality assessment. Although neither model managed to produce correct responses as per the binary correctness evaluation in this benchmarking run, the higher RewardScore for the thinking model suggests it is better aligned with our desired output characteristics. This validates the approach of incentivizing extended reasoning during training, as it helps the model to combine a more robust thinking process with a relevant and solution-oriented output.

#figure(
align(center)[
 #block(
 width: auto,
 inset: 10pt,
 fill: luma(240),
 radius: 4pt,
 align(left)[ ```
  Correct: 0
  Wrong: 120
  Score: 0.0
  RewardScore: 0.22991768664088924
 ```
 ]
 )
], caption: "Benchmark output for the Qwen_2.5_7B_ARC_v0.2_thinking model"
) <exmaple-out-qwen_rhinking>

== Conclusion
The benchmarking phase serves as a critical step in validating the effectiveness of our reinforcement learning approach. By isolating the evaluation of content from syntax and concentrating on the factual correctness of the output, we ensure that the models are not only well-trained in generating fluent language but also in delivering reliable and relevant information.

The dual metrics—a detailed content reward score and a clear percentage of correct responses—provide a robust framework for continuous evaluation and future improvements. This structured approach to benchmarking not only confirms the current state of our models but also lays the groundwork for ongoing optimizations in response to real-world challenges.
]