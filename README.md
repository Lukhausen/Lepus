# Exploring the Impact of Synthetic Chain-of-Thought Fine-Tuning on LLM Reasoning Abilities

This repository contains the research and code for the project titled **"Exploring the Impact of Synthetic Chain-of-Thought Fine-Tuning on LLM Reasoning Abilities"**, conducted as part of out T3000 project at the Cooperative State University Baden-WÃ¼rttemberg (DHBW).

## Project Overview

Large Language Models (LLMs) have made significant advancements in natural language processing and problem-solving. However, their reasoning abilities, especially in scenarios that require multi-step thinking, remain a challenge. This project aims to explore whether fine-tuning LLMs using synthetic Chain-of-Thought (CoT) data can enhance their reasoning performance.

### Key Objectives
- Create a synthetic dataset of reasoning tasks that are solved step by step. (Enable Backtracking afteer Faliure)
- Fine-tune a Large Language Model (preferably LLaMA 70B to be open source) using the synthetic dataset.
- Evaluate the fine-tuned model on reasoning benchmarks and compare the results to un-fine tunted model.

## Workflow

1. [Research Benchmarking Methods](#research-benchmarking-methods

### Research Benchmarking Methods
